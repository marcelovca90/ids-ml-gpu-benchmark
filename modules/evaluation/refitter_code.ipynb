{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0dbe0",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_path = '../../2025-07-05/Input_Multiclass/KDD_Cup_1999_Multiclass.parquet'\n",
    "output_folder = \"../../2025-07-05/Output_Multiclass_Refitter_Temp/KDD_Cup_1999_Multiclass/hpo_pareto_refit\"\n",
    "target_column = 'label'\n",
    "handle_object_cols = 'keep'\n",
    "sampling_rate_global = None # 0.10\n",
    "sampling_rate_sets = 0.10\n",
    "sample_sets = ['train']\n",
    "min_samples_per_class = 1\n",
    "feature_selection_threshold = 0.99\n",
    "sample_filtering_quantile = 0.10\n",
    "hpo_n_trials = 10 # 1000\n",
    "hpo_timeout = 60 # 3600\n",
    "num_boost_round = 100 # 500\n",
    "early_stopping_rounds = 10 # 50\n",
    "n_jobs = -1\n",
    "random_state = 42\n",
    "plot_param_importances = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8bcd5",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optuna.exceptions import ExperimentalWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa25f30-c577-4bc0-ace4-d8cf91d15b18",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a877e4-d4e2-408f-9ec8-7acf40ea9cf0",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(random_state)\n",
    "cp.random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# Step 1: read dataset from parquet file\n",
    "df_full = cudf.read_parquet(dataset_path)\n",
    "\n",
    "col_info = {str(col): {} for col in df_full.columns}\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b639161-1f7f-40c2-b994-f6e55f5f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b4fc2-bffa-4996-96e6-651e8b238a63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 2: sample dataset if specified\n",
    "if sampling_rate_global:\n",
    "    df_full = df_full.sample(frac=sampling_rate_global, random_state=random_state)\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_1': df_full[col].nunique(), 'dtype_1': df_full[col].dtype, 'mem_1': df_full[col].memory_usage() / 1024**2})\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b777a-a275-4648-9da8-b7aeda1d349a",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: drop object columns if specified\n",
    "if handle_object_cols != 'keep':\n",
    "    object_cols = df_full.drop(columns=[target_column]).select_dtypes(include='object').columns    \n",
    "    if handle_object_cols == 'drop' and len(object_cols) > 0:\n",
    "        df_full = df_full.drop(columns=object_cols)\n",
    "    elif handle_object_cols == 'encode' and len(object_cols) > 0:\n",
    "        for col in object_cols:\n",
    "            n_unique = df_full[col].nunique()\n",
    "            if n_unique <= 1000:\n",
    "                df_full[col] = df_full[col].astype('category')\n",
    "            else:\n",
    "                col_str = df_full[col].astype(str)\n",
    "                hashed = col_str.hash_values(seed=seed).astype('int64') + seed\n",
    "                df_full[col] = (hashed % 1024).astype('int16')\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_2': df_full[col].nunique(), 'dtype_2': df_full[col].dtype, 'mem_2': df_full[col].memory_usage() / 1024**2})\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d2aa1-5f4b-4237-87b7-4dd2477579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(dataset_path.replace('.parquet', '.json'), 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Step 4: restore dtypes for other columns from metadata\n",
    "for col, dtype in metadata[\"dtypes\"].items():\n",
    "    if col in df_full.columns:# and col not in object_cols:\n",
    "        df_full[col] = df_full[col].astype(dtype)\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_3': df_full[col].nunique(), 'dtype_3': df_full[col].dtype, 'mem_3': df_full[col].memory_usage() / 1024**2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a741d-f8b2-434c-b6d7-ee68c06a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(col_info).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3736105-f7b1-4b57-a1b8-a9d20016bb15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_full.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5ded8-a1ce-4547-a46c-c2173f4d214e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_full[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5e122",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Factorize target column\n",
    "df_full[target_column], unique_values = df_full[target_column].factorize()\n",
    "assert df_full[target_column].nunique() >= 2, \"Classification requires two or more classes.\"\n",
    "label_to_index = {value: i for i, value in enumerate(unique_values.to_pandas())}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "labels = list(label_to_index.keys())\n",
    "df_full[target_column] = df_full[target_column].astype('int8')\n",
    "\n",
    "# Detect and assign codes to categorical columns\n",
    "numeric_columns = df_full.drop(columns=[target_column]).select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = df_full.drop(columns=[target_column]).select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "df_full[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import train_test_split\n",
    "\n",
    "def ensure_min_samples_per_class(df, stratify_col, min_samples_per_class, random_state):\n",
    "    \"\"\"Ensures that each class has at least `min_samples_per_class` samples using oversampling.\"\"\"\n",
    "    class_counts = df[stratify_col].value_counts().to_pandas()\n",
    "\n",
    "    # Oversample minority classes\n",
    "    oversampled = [\n",
    "        df[df[stratify_col] == c].sample(n=min_samples_per_class, replace=True, random_state=random_state)\n",
    "        for c in class_counts[class_counts < min_samples_per_class].index\n",
    "    ]\n",
    "    df_oversampled = cudf.concat([df] + oversampled, ignore_index=True) if oversampled else df\n",
    "\n",
    "    return df_oversampled.reset_index(drop=True)  # Fix index mismatch\n",
    "\n",
    "def restore_dtypes(df, dtypes):\n",
    "    for col, dtype in dtypes.to_dict().items():\n",
    "        df[col] = df[col].astype(dtype)\n",
    "\n",
    "def assign_subsets(df, stratify_col, train_frac, val_frac, test_frac, random_state):\n",
    "    \"\"\"Splits data into mutually exclusive train/val/test subsets before applying stratified sampling.\"\"\"\n",
    "    assert train_frac + val_frac + test_frac == 1.0, \"Fractions must sum to 1\"\n",
    "\n",
    "    # Assign subset labels\n",
    "    df_train, df_temp = train_test_split(df, test_size=(1 - train_frac), stratify=df[stratify_col], random_state=random_state)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=(test_frac / (val_frac + test_frac)), stratify=df_temp[stratify_col], random_state=random_state)\n",
    "\n",
    "    df_train[\"subset\"] = \"train\"\n",
    "    df_val[\"subset\"] = \"val\"\n",
    "    df_test[\"subset\"] = \"test\"\n",
    "\n",
    "    return cudf.concat([df_train, df_val, df_test]).reset_index(drop=True)\n",
    "\n",
    "def sample_group(x):\n",
    "    \"\"\"Helper function to stratify sample while ensuring class presence.\"\"\"\n",
    "    n_samples = max(min_samples_per_class, int(len(x) * sampling_rate_sets))\n",
    "    return x.sample(n=n_samples, replace=len(x) < n_samples, random_state=random_state)\n",
    "\n",
    "def stratified_sample(df, stratify_col, sample_sets, sampling_rate_sets, min_samples_per_class, random_state):\n",
    "    \"\"\"Applies stratified sampling while ensuring minimum samples per class, only for selected subsets.\"\"\"\n",
    "\n",
    "    # Apply stratified sampling only for the requested subsets\n",
    "    df_sampled = df.groupby([\"subset\", stratify_col], group_keys=False).apply(\n",
    "        lambda x: sample_group(x) if x[\"subset\"].iloc[0] in sample_sets else x\n",
    "    )\n",
    "\n",
    "    return df_sampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3affed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtypes_before = df_full.dtypes.copy(deep=True).sort_index()\n",
    "\n",
    "# Ensure enough samples before splitting\n",
    "split_ok = False\n",
    "while not split_ok:\n",
    "    df_full = ensure_min_samples_per_class(df_full, target_column, min_samples_per_class, random_state)\n",
    "    df_full = df_full.reset_index(drop=True)  # Reset index before splitting\n",
    "    df_dtypes_backup = df_full.dtypes.copy(deep=True)\n",
    "\n",
    "    # Factorize for stratification\n",
    "    category_mappings = {}\n",
    "\n",
    "    for col, dtype in df_full.dtypes.to_dict().items():\n",
    "        if dtype == 'category':\n",
    "            codes, uniques = df_full[col].factorize()\n",
    "            df_full[col] = codes.astype('int32')\n",
    "            category_mappings[col] = uniques.to_pandas().tolist()\n",
    "\n",
    "    try:\n",
    "        # Assign mutually exclusive train/val/test subsets\n",
    "        df_full = assign_subsets(df_full, \"label\", train_frac=0.6, val_frac=0.2, test_frac=0.2, random_state=random_state)\n",
    "        split_ok = True  # If it succeeds, exit loop\n",
    "    except ValueError as e:\n",
    "        print(f\"Resampling due to insufficient class representation (min_samples_per_class={min_samples_per_class})...\")\n",
    "        min_samples_per_class += 1  # Increment dynamically and retry\n",
    "    \n",
    "    for col, dtype in df_dtypes_backup.items():\n",
    "        if col in category_mappings:\n",
    "            # Restore category values from factorized codes\n",
    "            mapping = dict(enumerate(category_mappings[col]))\n",
    "            df_full[col] = df_full[col].map(mapping).astype('category')\n",
    "        else:\n",
    "            # Restore other dtypes (numeric, bool, etc.)\n",
    "            df_full[col] = df_full[col].astype(dtype)\n",
    "\n",
    "print(f\"Minimal oversampling completed successfully (min_samples_per_class={min_samples_per_class}).\")\n",
    "\n",
    "df_full[target_column].value_counts()\n",
    "\n",
    "df_dtypes_after = df_full.dtypes.copy(deep=True).drop('subset').sort_index()\n",
    "\n",
    "assert df_dtypes_before.equals(df_dtypes_after), \"Dtype mismatch after assigning subsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4bd60-b3fd-4776-a522-5ab06e58bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values\n",
    "def fill_categorical_nas(dfs):\n",
    "    return\n",
    "    for df in dfs:\n",
    "        for col in df.select_dtypes(include=['category']).columns:\n",
    "            if df[col].to_pandas().isna().sum() > 0:\n",
    "                df[col] = cudf.Series(df[col].to_pandas().astype(str).replace(\"nan\", \"missing\").astype(\"category\"))\n",
    "\n",
    "# Extract final datasets\n",
    "df_train_full = df_full[df_full[\"subset\"] == \"train\"].drop(columns=[\"subset\"])\n",
    "df_val_full = df_full[df_full[\"subset\"] == \"val\"].drop(columns=[\"subset\"])\n",
    "df_test_full = df_full[df_full[\"subset\"] == \"test\"].drop(columns=[\"subset\"])\n",
    "\n",
    "# Convert back to X, y format\n",
    "X_train_full, y_train_full = df_train_full.drop(columns=[target_column]), df_train_full[target_column]\n",
    "X_val_full, y_val_full = df_val_full.drop(columns=[target_column]), df_val_full[target_column]\n",
    "X_test_full, y_test_full = df_test_full.drop(columns=[target_column]), df_test_full[target_column]\n",
    "\n",
    "fill_categorical_nas([X_train_full, X_val_full, X_test_full])\n",
    "\n",
    "# Ensure disjoint splits\n",
    "assert set(X_train_full.to_pandas().index).isdisjoint(set(X_val_full.to_pandas().index)), \"Train and Validation sets are not disjoint!\"\n",
    "assert set(X_train_full.to_pandas().index).isdisjoint(set(X_test_full.to_pandas().index)), \"Train and Test sets are not disjoint!\"\n",
    "assert set(X_val_full.to_pandas().index).isdisjoint(set(X_test_full.to_pandas().index)), \"Validation and Test sets are not disjoint!\"\n",
    "\n",
    "# Ensure all classes are present in each split\n",
    "assert y_train_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Train!\"\n",
    "assert y_val_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Validation!\"\n",
    "assert y_test_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Test!\"\n",
    "\n",
    "# Print final distributions\n",
    "print(f\"Train      : {len(df_train_full)} samples ({(100.0 * len(df_train_full) / len(df_full)):.2f}%), {y_train_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")\n",
    "print(f\"Validation : {len(df_val_full)} samples ({(100.0 * len(df_val_full) / len(df_full)):.2f}%), {y_val_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")\n",
    "print(f\"Test       : {len(df_test_full)} samples ({(100.0 * len(df_test_full) / len(df_full)):.2f}%), {y_test_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17bb2a-7390-4246-870c-0f8425a01a7f",
   "metadata": {},
   "source": [
    "# Step 2: Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea6e59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, cohen_kappa_score,\n",
    "    log_loss, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "num_classes = df_full[target_column].nunique()\n",
    "loss_function = 'binary:logistic' if num_classes == 2 else 'multi:softprob'\n",
    "eval_metric = 'logloss' if num_classes == 2 else 'mlogloss'\n",
    "n_folds, shuffle, stratify = 5, True, True\n",
    "\n",
    "def evaluate_multiclass_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_prob=None,\n",
    "    labels=None,\n",
    "    label_names=None\n",
    "):\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"balanced_accuracy\"] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"cohen_kappa\"] = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:\n",
    "        metrics[f\"precision_{avg}\"] = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "        metrics[f\"recall_{avg}\"] = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "        metrics[f\"f1_{avg}\"] = f1_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            metrics[\"log_loss\"] = log_loss(y_true, y_prob, labels=labels)\n",
    "        except:\n",
    "            metrics[\"log_loss\"] = np.nan\n",
    "\n",
    "    if y_prob is not None and labels is not None and len(np.unique(y_true)) > 1:\n",
    "        for avg in [\"micro\", \"macro\", \"weighted\"]:\n",
    "            try:\n",
    "                metrics[f\"roc_auc_ovr_{avg}\"] = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=avg, labels=labels)\n",
    "                metrics[f\"roc_auc_ovo_{avg}\"] = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=avg, labels=labels)\n",
    "            except:\n",
    "                metrics[f\"roc_auc_ovr_{avg}\"] = np.nan\n",
    "                metrics[f\"roc_auc_ovo_{avg}\"] = np.nan\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_true)\n",
    "    conf = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    display_labels = label_names if label_names is not None else labels\n",
    "    conf_df = pd.DataFrame(conf, index=[f\"True_{l}\" for l in display_labels],\n",
    "                                 columns=[f\"Pred_{l}\" for l in display_labels])\n",
    "\n",
    "    return metrics, conf_df\n",
    "\n",
    "def f1_weighted_eval(preds, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    weights = dtrain.get_weight()\n",
    "    # Convert probabilities to class labels\n",
    "    if num_classes > 2:\n",
    "        y_pred = np.argmax(preds.reshape(y_true.shape[0], -1), axis=1)\n",
    "    else:\n",
    "        y_pred = (preds > 0.5).astype(int)\n",
    "    # Use weights if provided\n",
    "    if weights is not None and len(weights) > 0:\n",
    "        f1 = f1_score(y_true, y_pred, average=\"weighted\", sample_weight=weights)\n",
    "    else:\n",
    "        f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    return \"f1_weighted\", f1\n",
    "\n",
    "def get_model_size(model):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".ubj\") as temp_model:\n",
    "        model_path = temp_model.name\n",
    "    model.save_model(model_path)\n",
    "    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    os.remove(model_path)\n",
    "    return round(model_size_mb, 2)\n",
    "\n",
    "# Default XGB Booster parameters\n",
    "default_booster_params = {\n",
    "    \"objective\": loss_function,                     # Multi-class classification\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of iterations\n",
    "    \"num_boost_round\": num_boost_round,             # Number of CV folds\n",
    "    \"eval_metric\": eval_metric,                     # Log loss\n",
    "    \"device\": 'cuda'                                # GPU (CUDA)\n",
    "}\n",
    "if num_classes > 2:\n",
    "    default_booster_params[\"num_class\"] = num_classes # Only for multi-class classification\n",
    "\n",
    "# Default XGB CV parameters\n",
    "default_cv_params = {\n",
    "    \"params\": default_booster_params,               # Booster parameters\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of rounds before early stopping\n",
    "    \"num_boost_round\": num_boost_round,             # Number of iterations\n",
    "    \"nfold\": n_folds,                               # Number of CV folds\n",
    "    \"shuffle\": shuffle,                             # Shuffle samples before creating folds\n",
    "    \"stratified\": stratify,                         # Stratify classes on CV split\n",
    "    \"metrics\": eval_metric,                         # Log loss\n",
    "    \"feval\": f1_weighted_eval,                      # Monitor weighted F1 score\n",
    "    \"seed\": random_state                            # Passed to numpy.random.seed\n",
    "}\n",
    "\n",
    "# Default XGB training parameters\n",
    "default_train_params = {\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of rounds before early stopping\n",
    "    \"num_boost_round\": num_boost_round,             # Number of iterations\n",
    "}\n",
    "\n",
    "# Function to Train, Validate, and Test XGBoost Model\n",
    "def train_xgb(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    sample_weights=None,\n",
    "    cv=False,\n",
    "    custom_booster_params=None,\n",
    "    metrics_filename: str = None,\n",
    "    cm_filename: str = None,\n",
    "    model_filename: str = None,\n",
    "    verbose=1\n",
    "):\n",
    "    # Warmup verification\n",
    "    assert sample_weights is None or len(sample_weights) == len(y_train), \"Sample weights must align with training labels\"\n",
    "\n",
    "    # Create DMatrix with feature names\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True, feature_names=X_train.columns.tolist(), weight=sample_weights)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True, feature_names=X_val.columns.tolist()) if X_val is not None else None\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=X_test.columns.tolist()) if X_test is not None else None\n",
    "\n",
    "    # Merge default booster params with custom ones\n",
    "    booster_params = {**default_booster_params, **(custom_booster_params or {})}\n",
    "\n",
    "    if cv:\n",
    "        cv_params = {\n",
    "            **default_cv_params,\n",
    "            \"params\": booster_params,\n",
    "            \"verbose_eval\": verbose\n",
    "        }\n",
    "        training_start = time.time()\n",
    "        results = xgb.cv(dtrain=dtrain, **cv_params)\n",
    "        training_end = time.time()\n",
    "        training_time = (training_end - training_start) / n_folds\n",
    "        latency = training_time / len(y_train)\n",
    "        f1_score_ans = float(results['test-f1_weighted-mean'].mean())\n",
    "        model, model_size = None, None\n",
    "\n",
    "        # Persist the model properly\n",
    "        if metrics_filename:\n",
    "            os.makedirs(os.path.dirname(metrics_filename), exist_ok=True)\n",
    "            results.to_csv(metrics_filename, index=True)\n",
    "\n",
    "    else:\n",
    "        training_start = time.time()\n",
    "        model = xgb.train(\n",
    "            params=booster_params,\n",
    "            dtrain=dtrain,\n",
    "            evals=[(dtrain, \"Train\"), (dval, \"Validation\")],\n",
    "            **default_train_params\n",
    "        )\n",
    "        training_end = time.time()\n",
    "        training_time = training_end - training_start\n",
    "\n",
    "        # Predict\n",
    "        test_start = time.time()\n",
    "        y_pred_probs = model.predict(dtest)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1) if num_classes > 2 else (y_pred_probs > 0.5).astype(int)\n",
    "        test_end = time.time()\n",
    "        test_time = test_end - test_start\n",
    "\n",
    "        latency = test_time / len(y_test)\n",
    "\n",
    "        model_size = get_model_size(model)\n",
    "\n",
    "        metrics_dict, confusion_df = evaluate_multiclass_metrics(\n",
    "            y_true=y_test.to_numpy(),\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_pred_probs,\n",
    "            labels=list(index_to_label.keys()),\n",
    "            label_names=[index_to_label[i] for i in index_to_label]\n",
    "        )\n",
    "        metrics_dict.update({\n",
    "            'training_time': training_time, 'test_time': test_time, 'latency': latency, 'model_size': model_size\n",
    "        })\n",
    "        \n",
    "        f1_score_ans = metrics_dict['f1_weighted']\n",
    "\n",
    "        # Persist the results properly\n",
    "        if metrics_filename:\n",
    "            os.makedirs(os.path.dirname(metrics_filename), exist_ok=True)\n",
    "            with open(metrics_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(metrics_dict, f, indent=2)\n",
    "        if cm_filename:\n",
    "            os.makedirs(os.path.dirname(cm_filename), exist_ok=True)\n",
    "            confusion_df.to_csv(cm_filename, index=True)\n",
    "        if model_filename:\n",
    "            os.makedirs(os.path.dirname(model_filename), exist_ok=True)\n",
    "            model.save_model(model_filename)\n",
    "\n",
    "    return model, training_time, latency, f1_score_ans, model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7c3f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# train_xgb(X_train_full, None, None, y_train_full, None, None, cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ffa1a-e27a-4032-8231-9efd4aaf5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgb(X_train_full, X_val_full, X_test_full, y_train_full, y_val_full, y_test_full, cv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6398b-ffe3-44ca-aa8b-6a6112fbfbc5",
   "metadata": {},
   "source": [
    "# Step 3: Baseline Evaluation (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a14a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "model_full, train_time_full, latency_full, f1_weighted_full, model_size_full = train_xgb(\n",
    "    X_train_full, X_val_full, X_test_full,\n",
    "    y_train_full, y_val_full, y_test_full,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_full_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_full_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_full_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (full): {train_time_full:.3f} seconds\")\n",
    "print(f\"Latency (full): {latency_full:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (full): {f1_weighted_full:.6f}\")\n",
    "print(f\"Model Size: {model_size_full} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b7b4c-5b79-46cc-8881-9cf6c8b25ed5",
   "metadata": {},
   "source": [
    "# Step 4: Updated Evaluation (with Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc95fe7-1d08-4874-8565-7b132525a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stratified sampling only to the selected subsets\n",
    "df_sampled = stratified_sample(df_full, target_column, sample_sets, sampling_rate_sets, min_samples_per_class, random_state)\n",
    "\n",
    "# Extract final datasets\n",
    "df_train_sampled = df_sampled[df_sampled[\"subset\"] == \"train\"].drop(columns=[\"subset\"])\n",
    "df_val_sampled = df_sampled[df_sampled[\"subset\"] == \"val\"].drop(columns=[\"subset\"])\n",
    "df_test_sampled = df_sampled[df_sampled[\"subset\"] == \"test\"].drop(columns=[\"subset\"])\n",
    "\n",
    "# Convert back to X, y format\n",
    "X_train_sampled, y_train_sampled = df_train_sampled.drop(columns=[target_column]), df_train_sampled[target_column]\n",
    "X_val_sampled, y_val_sampled = df_val_sampled.drop(columns=[target_column]), df_val_sampled[target_column]\n",
    "X_test_sampled, y_test_sampled = df_test_sampled.drop(columns=[target_column]), df_test_sampled[target_column]\n",
    "\n",
    "fill_categorical_nas([X_train_sampled, X_val_sampled, X_test_sampled])\n",
    "\n",
    "# Ensure disjoint splits\n",
    "assert set(X_train_sampled.to_pandas().index).isdisjoint(set(X_val_sampled.to_pandas().index)), \"Train and Validation sets are not disjoint!\"\n",
    "assert set(X_train_sampled.to_pandas().index).isdisjoint(set(X_test_sampled.to_pandas().index)), \"Train and Test sets are not disjoint!\"\n",
    "assert set(X_val_sampled.to_pandas().index).isdisjoint(set(X_test_sampled.to_pandas().index)), \"Validation and Test sets are not disjoint!\"\n",
    "\n",
    "# Ensure all classes are present in each split\n",
    "assert y_train_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Train!\"\n",
    "assert y_val_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Validation!\"\n",
    "assert y_test_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Test!\"\n",
    "\n",
    "# Print final distributions\n",
    "print(f\"Train      : {len(df_train_sampled)} samples ({(100.0 * len(df_train_sampled) / len(df_full)):.2f}%), {y_train_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")\n",
    "print(f\"Validation : {len(df_val_sampled)} samples ({(100.0 * len(df_val_sampled) / len(df_full)):.2f}%), {y_val_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")\n",
    "print(f\"Test       : {len(df_test_sampled)} samples ({(100.0 * len(df_test_sampled) / len(df_full)):.2f}%), {y_test_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16e1b7-6372-45e6-81bc-94eb10754319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "model_sampled, train_time_sampled, latency_sampled, f1_weighted_sampled, model_size_sampled = train_xgb(\n",
    "    X_train_sampled, X_val_sampled, X_test_sampled,\n",
    "    y_train_sampled, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_sampled_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_sampled_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_sampled_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (sampled): {train_time_sampled:.3f} seconds\")\n",
    "print(f\"Latency (sampled): {latency_sampled:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (sampled): {f1_weighted_sampled:.6f}\")\n",
    "print(f\"Model Size: {model_size_sampled} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a82ddb-cb5c-42d4-b6b4-808f097c0c62",
   "metadata": {},
   "source": [
    "# Step 5: Updated Evaluation (with Sampling and Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89e6ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate and sort features by importance\n",
    "feature_importance = model_sampled.get_score(importance_type=\"gain\")\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "features, importance = zip(*sorted_features)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "bars = plt.barh(features, importance, color='skyblue')\n",
    "\n",
    "# Annotate each bar with its importance value\n",
    "for bar, value in zip(bars, importance):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\" {value:.3f}\", va='center')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Gain (Importance)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)  # Add grid for better readability\n",
    "\n",
    "plt.savefig(f\"{output_folder}/xgb_feature_importance_gain.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c66cf-7f00-469a-b210-eb8f466fa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model_sampled.get_score(importance_type=\"gain\")\n",
    "importance_df = cudf.DataFrame(list(feature_importance.items()), columns=[\"Feature\", \"Importance\"])\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Normalize importance scores\n",
    "importance_df[\"Cumulative_Importance\"] = importance_df[\"Importance\"].cumsum() / importance_df[\"Importance\"].sum()\n",
    "\n",
    "# Find the smallest set of features that explains at least 95% of the importance\n",
    "N = np.argmax(importance_df[\"Cumulative_Importance\"] >= feature_selection_threshold) + 1  # Adjust threshold as needed\n",
    "\n",
    "# Select top features\n",
    "selected_features = importance_df[\"Feature\"][:N].to_pandas().tolist()\n",
    "\n",
    "print(f\"Optimal number of features: {N} (from {len(feature_importance)})\")\n",
    "\n",
    "importance_df.to_csv(f\"{output_folder}/xgb_feature_importance_gain.csv\", index=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8e11b-daee-4999-bfa7-6d83e6603780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the feature space for train, validation, and test sets\n",
    "X_train_reduced = X_train_sampled[selected_features]\n",
    "X_val_reduced = X_val_sampled[selected_features]\n",
    "X_test_reduced = X_test_sampled[selected_features]\n",
    "\n",
    "fill_categorical_nas([X_train_reduced, X_val_reduced, X_test_reduced])\n",
    "\n",
    "# Train and evaluate using train_xgb function\n",
    "model_reduced, train_time_reduced, latency_reduced, f1_weighted_reduced, model_size_reduced = train_xgb(\n",
    "    X_train_reduced, X_val_reduced, X_test_reduced,\n",
    "    y_train_sampled, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_reduced_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_reduced_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_reduced_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (reduced): {train_time_reduced:.3f} seconds\")\n",
    "print(f\"Latency (reduced): {latency_reduced:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (reduced): {f1_weighted_reduced:.6f}\")\n",
    "print(f\"Model Size (reduced): {model_size_reduced} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15b394-b297-4511-88d8-4c77f78bc5bb",
   "metadata": {},
   "source": [
    "# Step 6: Updated Evaluation (with Sampling, Feature Selection, and Row Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1424a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def convert_categorical_to_frequency(df, normalize=True):\n",
    "    \"\"\"Convert categorical features to frequency encoding with reversibility support.\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    category_mappings = {}\n",
    "    row_mappings = {}\n",
    "\n",
    "    for col in df_encoded.select_dtypes(include=['category']).columns:\n",
    "        # Calculate normalized frequencies\n",
    "        freqs = df[col].to_pandas().value_counts().astype('float32')\n",
    "        if normalize:\n",
    "            freqs = freqs / len(df)\n",
    "        freq_map = freqs.to_dict()\n",
    "\n",
    "        # Encode column\n",
    "        df_encoded[col] = df[col].to_pandas().map(freq_map).astype('float32')\n",
    "\n",
    "        # Save both mappings\n",
    "        category_mappings[col] = freq_map\n",
    "        row_mappings[col] = df[col].reset_index(drop=True)\n",
    "\n",
    "    return df_encoded, category_mappings, row_mappings\n",
    "\n",
    "\n",
    "def revert_frequency_encoding(df_encoded, row_mappings):\n",
    "    \"\"\"Revert frequency-encoded DataFrame to original categories using stored row-wise values.\"\"\"\n",
    "    df_reverted = df_encoded.copy().reset_index(drop=True)\n",
    "\n",
    "    for col in row_mappings:\n",
    "        if col in df_reverted.columns:\n",
    "            df_reverted[col] = row_mappings[col]  # restore from stored original\n",
    "\n",
    "    return df_reverted\n",
    "\n",
    "\n",
    "def filter_low_mean_samples(X_encoded: cudf.DataFrame, y: cudf.Series, quantile_threshold: float):\n",
    "    \"\"\"Filter low-mean samples per class while preserving all class labels.\"\"\"\n",
    "    \n",
    "    # Step 1: Combine features and label\n",
    "    combined_df = X_encoded.copy()\n",
    "    combined_df[target_column] = y\n",
    "\n",
    "    # Step 2: Compute row means once\n",
    "    row_means = X_encoded.mean(axis=1)\n",
    "    combined_df['row_mean'] = row_means\n",
    "\n",
    "    # Step 3: Compute quantile thresholds per class\n",
    "    class_thresholds = (\n",
    "        combined_df[[target_column, 'row_mean']]\n",
    "        .groupby(target_column)\n",
    "        .quantile(q=quantile_threshold)\n",
    "        .rename(columns={'row_mean': 'threshold'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Step 4: Join thresholds back to combined_df\n",
    "    combined_df = combined_df.merge(class_thresholds, on=target_column, how='left')\n",
    "\n",
    "    # Step 5: Apply filtering\n",
    "    filtered_df = combined_df[combined_df['row_mean'] > combined_df['threshold']]\n",
    "\n",
    "    # Step 6: Ensure all classes are represented\n",
    "    present_classes = set(cp.asnumpy(filtered_df[target_column].unique()))\n",
    "    all_classes = set(cp.asnumpy(y.unique()))\n",
    "    missing_classes = all_classes - present_classes\n",
    "\n",
    "    for label in missing_classes:\n",
    "        class_df = combined_df[combined_df[target_column] == label]\n",
    "        best_idx = class_df['row_mean'].to_pandas().idxmax()\n",
    "        best_sample = class_df.loc[[best_idx]]\n",
    "        filtered_df = cudf.concat([filtered_df, best_sample])\n",
    "\n",
    "    # Step 7: Final separation\n",
    "    X_filtered = filtered_df.drop([target_column, 'row_mean', 'threshold'], axis=1)\n",
    "    y_filtered = filtered_df[target_column]\n",
    "\n",
    "    return X_filtered, y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply frequency encoding to the training set\n",
    "print('convert_categorical_to_frequency')\n",
    "X_train_filtered_encoded, cat_map, row_map = convert_categorical_to_frequency(X_train_reduced)\n",
    "\n",
    "# Apply per-class row filtering\n",
    "print('filter_low_mean_samples')\n",
    "X_train_filtered, y_train_filtered = filter_low_mean_samples(\n",
    "    X_train_filtered_encoded, \n",
    "    y_train_sampled, \n",
    "    sample_filtering_quantile\n",
    ")\n",
    "\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns)\n",
    "\n",
    "# Revert frequency encoding\n",
    "print('revert_frequency_encoding')\n",
    "# Subset row_map to match the filtered row indices\n",
    "filtered_row_map = {col: row_map[col].loc[X_train_filtered.index] for col in row_map}\n",
    "X_train_filtered = revert_frequency_encoding(X_train_filtered, row_map)\n",
    "fill_categorical_nas([X_train_filtered])\n",
    "\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original training set size: {len(X_train_reduced)} rows\")\n",
    "print(f\"Filtered training set size: {len(X_train_filtered)} rows\")\n",
    "\n",
    "print(f\"\\nClass distribution before filtering:\")\n",
    "print(y_train_sampled.value_counts().sort_index())\n",
    "print(f\"\\nClass distribution after filtering:\")\n",
    "print(y_train_filtered.value_counts().sort_index())\n",
    "\n",
    "# Final validations\n",
    "assert len(X_train_filtered) == len(y_train_filtered), \"Sample count mistmatch between X and y\"\n",
    "assert len(X_train_filtered.columns) == len(X_train_reduced.columns), \"Column count changed\"\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns), \"Column names changed\"\n",
    "assert y_train_filtered.nunique() == y_train_sampled.nunique(), \"Class count changed\"\n",
    "assert y_train_filtered.nunique() == df_full[target_column].nunique(), \"Some classes are missing\"\n",
    "assert all(y_train_filtered.to_pandas().value_counts() >= 1), \"Some classes have zero samples\"\n",
    "assert X_train_filtered.index.is_unique, \"Duplicate indices in filtered data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "clf_filtered, train_time_filtered, latency_filtered, f1_weighted_filtered, model_size_filtered = train_xgb(\n",
    "    X_train_filtered, X_val_reduced, X_test_reduced,\n",
    "    y_train_filtered, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_filtered_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_filtered_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_filtered_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (filtered): {train_time_filtered:.3f} seconds\")\n",
    "print(f\"Latency (filtered): {latency_filtered:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (filtered): {f1_weighted_filtered:.6f}\")\n",
    "print(f\"Model Size (filtered): {model_size_filtered} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cd99c-57fd-4616-81f9-50a9329aeb6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Step 7: HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52c600-514b-4f36-adb5-b41ffbd48d00",
   "metadata": {},
   "source": [
    "## Step 7.1: Numeric Scaling Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba930803-3ceb-43b3-a612-566378268917",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "scaling_methods = ['none', 'maxabs', 'minmax', 'norm', 'robust', 'standard'] if numeric_columns else ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8e09e-9bb3-429a-9bcd-401767bc55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import MaxAbsScaler, MinMaxScaler, Normalizer, RobustScaler, StandardScaler\n",
    "\n",
    "def make_numeric_scaler(scaling_method, X):\n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "    non_numeric_columns = X.columns.difference(numeric_columns)\n",
    "\n",
    "    scalers = {\n",
    "        'none': None,\n",
    "        'maxabs': MaxAbsScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'norm': Normalizer(),\n",
    "        'robust': RobustScaler(),\n",
    "        'standard': StandardScaler()\n",
    "    }\n",
    "\n",
    "    scaler = scalers.get(scaling_method)\n",
    "\n",
    "    def scale_numeric_features(X_input, fit=False):\n",
    "        X_numeric_pd = X_input[numeric_columns].copy()\n",
    "        X_non_numeric_pd = X_input[non_numeric_columns].copy()\n",
    "        X_output = None\n",
    "\n",
    "        if scaling_method == 'none':\n",
    "            X_output = X_input\n",
    "        else:\n",
    "            if fit:\n",
    "                scaler.fit(X_numeric_pd)\n",
    "            X_numeric_scaled_pd = scaler.transform(X_numeric_pd)\n",
    "            X_numeric_scaled_cu = X_numeric_scaled_pd.astype('float32') # cudf.from_pandas(X_numeric_scaled_pd)\n",
    "            X_numeric_scaled_cu.columns = list(numeric_columns)\n",
    "            X_numeric_scaled_cu.index = X_input.index\n",
    "            # X_numeric_scaled_cu = X_numeric_scaled_cu.reset_index(drop=True)\n",
    "            X_non_numeric_cu = X_non_numeric_pd # cudf.from_pandas(X_non_numeric_pd)\n",
    "            X_non_numeric_cu.columns = list(non_numeric_columns)\n",
    "            X_non_numeric_cu.index = X_input.index\n",
    "            # X_non_numeric_cu = X_non_numeric_cu.reset_index(drop=True)\n",
    "            X_output = cudf.concat([X_numeric_scaled_cu, X_non_numeric_cu], axis=1)\n",
    "            assert X_input.shape == X_output.shape, \"Shape mismatch after concat.\"\n",
    "\n",
    "        return X_output\n",
    "\n",
    "    return scale_numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc40d7-8aa4-4a77-a6a0-505cbfa10187",
   "metadata": {},
   "source": [
    "## Step 7.2: Categorical Encoding Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb9de6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "encoding_methods = ['none', 'onehot', 'ordinal', 'frequency'] if categorical_cols else ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import LabelEncoder as CumlLabelEncoder\n",
    "from cuml.preprocessing import OneHotEncoder as CumlOneHotEncoder\n",
    "\n",
    "# Define categorical encoding function\n",
    "def make_categorical_encoder(encoding_method, X, max_categories_for_onehot=10):\n",
    "    \"\"\"Creates an encoder for categorical features while keeping numerical columns unchanged.\"\"\"\n",
    "\n",
    "    categorical_columns = X.select_dtypes(include=['category']).columns\n",
    "    numeric_columns = X.columns.difference(categorical_columns)  # Numeric columns remain unchanged\n",
    "\n",
    "    # Split categorical columns for One-Hot vs. None Encoding\n",
    "    onehot_columns = [col for col in categorical_columns if X[col].nunique() <= max_categories_for_onehot]\n",
    "    none_columns = list(set(categorical_columns) - set(onehot_columns))  # High-cardinality features\n",
    "\n",
    "    # Ensure onehot_columns is a valid list (avoiding Index issues)\n",
    "    onehot_columns = list(onehot_columns) if hasattr(onehot_columns, 'tolist') else onehot_columns\n",
    "\n",
    "    # Define encoding strategies\n",
    "    encoders = {\n",
    "        'none': None,   # No transformation (Handled separately)\n",
    "        'onehot': CumlOneHotEncoder(handle_unknown='ignore', sparse=False, sparse_output=False),\n",
    "        'ordinal': CumlLabelEncoder(handle_unknown='ignore'),\n",
    "        'frequency': None  # Frequency encoding requires custom logic\n",
    "    }\n",
    "\n",
    "    encoder = encoders.get(encoding_method)\n",
    "\n",
    "    # Store frequency map for reuse\n",
    "    frequency_col_map, frequency_row_map = {}, {}\n",
    "\n",
    "    def encode_categorical_features(X_input, fit=False):\n",
    "        nonlocal encoder\n",
    "        \"\"\"Encodes categorical features while keeping numeric features unchanged.\"\"\"\n",
    "        _categorical_columns = X_input.select_dtypes(include=['category']).columns\n",
    "        _numeric_columns = X_input.columns.difference(_categorical_columns)\n",
    "        X_numeric = X_input[_numeric_columns].copy()\n",
    "        X_categorical = X_input[_categorical_columns].copy()\n",
    "\n",
    "        if encoding_method == 'none':\n",
    "            # Convert categorical columns to category dtype and assign codes\n",
    "            # X_categorical = X_categorical.apply(lambda col: col.astype('category').cat.codes)\n",
    "            return X_input\n",
    "\n",
    "        elif encoding_method == 'onehot':\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "            # Handle categorical columns together for cuML\n",
    "            if fit:\n",
    "                encoder.fit(X_categorical[onehot_columns])\n",
    "            X_onehot = cudf.DataFrame(\n",
    "                data=encoder.transform(X_categorical[onehot_columns]).astype('int8'), \n",
    "                columns=encoder.get_feature_names(onehot_columns)\n",
    "            )\n",
    "            # Ensure index alignment\n",
    "            X_onehot.index = X_numeric_copy.index  \n",
    "            # Add encoded columns to result\n",
    "            X_encoded = cudf.concat([X_numeric_copy, X_onehot], axis=1)\n",
    "\n",
    "            # Apply 'none' encoding (category codes) for high-cardinality features\n",
    "            if none_columns:\n",
    "                # X_none_encoded = X_categorical[none_columns].apply(lambda col: col.cat.codes)\n",
    "                X_encoded = cudf.concat([X_encoded, X_input[none_columns]], axis=1)\n",
    "\n",
    "            return X_encoded\n",
    "\n",
    "        elif encoding_method == 'ordinal':\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "            X_ordinal_partials = []\n",
    "            # Handle each categorical column separately for cuML\n",
    "            for col in X_categorical.columns:\n",
    "                if fit:\n",
    "                    encoder.fit(X_categorical[col])\n",
    "                X_ordinal = cudf.DataFrame(\n",
    "                    data=encoder.transform(X_categorical[col]).astype('category'),\n",
    "                    columns=[col]\n",
    "                )\n",
    "                # Ensure index alignment\n",
    "                X_ordinal.index = X_numeric_copy.index  \n",
    "                # Add encoded columns to result\n",
    "                X_ordinal_partials.append(X_ordinal)\n",
    "            X_encoded = cudf.concat([X_numeric_copy, cudf.concat(X_ordinal_partials, axis=1)], axis=1)\n",
    "\n",
    "            return X_encoded\n",
    "\n",
    "        elif encoding_method == 'frequency':\n",
    "            nonlocal frequency_col_map, frequency_row_map\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "\n",
    "            if fit:\n",
    "                # Fit and encode\n",
    "                X_categorical_encoded, frequency_col_map, frequency_row_map = \\\n",
    "                    convert_categorical_to_frequency(X_categorical)\n",
    "            else:\n",
    "                # Transform using saved frequency map\n",
    "                X_categorical_encoded = X_categorical.copy()\n",
    "                for col in X_categorical.columns:\n",
    "                    mapped = X_categorical[col].to_pandas().map(frequency_col_map.get(col, {}))\n",
    "                    X_categorical_encoded[col] = cudf.Series(mapped, index=X_categorical.index).astype('float32')\n",
    "\n",
    "            return cudf.concat([X_numeric_copy, X_categorical_encoded], axis=1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n",
    "\n",
    "        # Ensure all categorical columns are replaced by their codes\n",
    "        # X_categorical = X_categorical.apply(lambda col: col.cat.codes)\n",
    "\n",
    "    return encode_categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15172bb4-7dab-4b34-a6b3-7732dbfbe7ab",
   "metadata": {},
   "source": [
    "## Step 7.3: Data Balancing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68333e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "SAFE_MODE = False\n",
    "\n",
    "def assert_or_raise(condition: bool, message: str):\n",
    "    if SAFE_MODE and not condition:\n",
    "        raise ValueError(message)\n",
    "\n",
    "class CumlRandomResampler:\n",
    "    def __init__(self, strategy: dict, sampling_type: str = 'over'):\n",
    "        assert sampling_type in ['over', 'under'], \"sampling_type must be 'over' or 'under'\"\n",
    "        self.strategy = strategy\n",
    "        self.sampling_type = sampling_type\n",
    "\n",
    "    def fit_resample(self, X: cudf.DataFrame, y: cudf.Series):\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "\n",
    "        df = X.copy()\n",
    "        df['__label__'] = y\n",
    "\n",
    "        resampled_dfs = []\n",
    "\n",
    "        for cls in df['__label__'].to_pandas().unique().tolist():\n",
    "            cls_df = df[df['__label__'] == cls]\n",
    "            count = len(cls_df)\n",
    "\n",
    "            if self.sampling_type == 'over' and cls in self.strategy:\n",
    "                target_total = self.strategy[cls]\n",
    "                current_count = len(cls_df)\n",
    "                to_add = target_total - current_count\n",
    "\n",
    "                if to_add > 0:\n",
    "                    extra = cls_df.sample(n=to_add, replace=True, random_state=42)\n",
    "                    cls_df = cudf.concat([cls_df, extra], ignore_index=True)\n",
    "                    assert_or_raise(cls_df.shape[0] == target_total,\n",
    "                                    f\"Over-sampling failed for class {cls}: expected {target_total}, got {cls_df.shape[0]}\")\n",
    "                    assert_or_raise(cls_df.shape[1] == df.shape[1],\n",
    "                                    f\"Column mismatch after over-sampling class {cls}\")\n",
    "\n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "            elif self.sampling_type == 'under' and cls in self.strategy:\n",
    "                target_keep = self.strategy[cls]\n",
    "                to_remove = count - target_keep\n",
    "\n",
    "                if to_remove >= count:\n",
    "                    # Skip appending — fully removed\n",
    "                    print(f\"[Resampler] Class {cls} will be removed entirely (requested to remove {to_remove}, only {count} available)\")\n",
    "                    continue\n",
    "\n",
    "                if to_remove > 0:\n",
    "                    cls_df = cls_df.sample(n=target_keep, replace=False, random_state=42)\n",
    "                    expected_rows = target_keep\n",
    "                    expected_cols = df[df['__label__'] == cls].shape[1]\n",
    "                    assert_or_raise(cls_df.shape[0] == expected_rows,\n",
    "                                    f\"Under-sampling failed for class {cls}: expected {expected_rows}, got {cls_df.shape[0]}\")\n",
    "                    assert_or_raise(cls_df.shape[1] == expected_cols,\n",
    "                                    f\"Column mismatch after under-sampling class {cls}\")\n",
    "                \n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "            else:\n",
    "                # Not in strategy → keep class as-is\n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "        result_df = cudf.concat(resampled_dfs, ignore_index=True)\n",
    "        X_resampled = result_df.drop(columns='__label__')\n",
    "        y_resampled = result_df['__label__']\n",
    "\n",
    "        # Final checks\n",
    "\n",
    "        expected_total = sum(self.strategy.values())\n",
    "        expected_classes = set(y.to_pandas().unique())\n",
    "        actual_classes = set(y_resampled.to_pandas().unique())\n",
    "        assert_or_raise(X_resampled.shape[0] == expected_total,\n",
    "                        f\"{self.sampling_type.capitalize()}-sampling total row count mismatch: expected {expected_total}, got {X_resampled.shape[0]}\")\n",
    "        assert_or_raise(y_resampled.shape[0] == expected_total,\n",
    "                        f\"{self.sampling_type.capitalize()}-sampling total label count mismatch: expected {expected_total}, got {y_resampled.shape[0]}\")\n",
    "        assert_or_raise(X_resampled.shape[1] == X.shape[1],\n",
    "                        f\"Feature count mismatch after resampling: expected {X.shape[1]}, got {X_resampled.shape[1]}\")\n",
    "        assert_or_raise(actual_classes == expected_classes,\n",
    "                        f\"Class mismatch after resampling: expected {expected_classes}, got {actual_classes}\")\n",
    "\n",
    "        return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721b475",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors as CumlNearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a822a-955d-4685-9c3e-d89b57407741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_resample(_X_train, _y_train, over_method, over_thresh, under_method, under_thresh):\n",
    "\n",
    "    _X_names = _X_train.columns.tolist()\n",
    "    _y_name = _y_train.name\n",
    "\n",
    "    _X_train_copy = _X_train.copy()\n",
    "    _y_train_copy = _y_train.copy()\n",
    "\n",
    "    if over_thresh:\n",
    "        value_counts = _y_train_copy.value_counts().to_dict()\n",
    "        n_neighbors = min(5, min(value_counts.values()))\n",
    "        n_generate = build_oversampling_strategy(value_counts, over_thresh)\n",
    "        over_strategy = patch_oversampling_strategy(value_counts, n_generate)\n",
    "        cat_features = [\n",
    "            _X_train_copy.columns.get_loc(col)\n",
    "            for col in _X_train_copy.select_dtypes(include=['category']).columns\n",
    "        ]\n",
    "        over_sampler = make_over_sampler(over_method, over_strategy, n_neighbors, cat_features)\n",
    "        _X_train_copy, _y_train_copy = over_sampler.fit_resample(_X_train_copy, _y_train_copy)\n",
    "\n",
    "    if under_thresh:\n",
    "        value_counts = _y_train_copy.value_counts().to_dict()\n",
    "        n_remove = build_undersampling_strategy(value_counts, under_thresh)\n",
    "        under_strategy = patch_undersampling_strategy(value_counts, n_remove)\n",
    "        under_sampler = make_under_sampler(under_method, under_strategy)\n",
    "        _X_train_copy, _y_train_copy = under_sampler.fit_resample(_X_train_copy, _y_train_copy)\n",
    "\n",
    "    return cudf.DataFrame(_X_train_copy, columns=_X_names), cudf.Series(_y_train_copy, name=_y_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282e984-eb6a-44d8-bb6b-131a2630e07e",
   "metadata": {},
   "source": [
    "### Step 7.3a: Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc68786",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "over_method_choices = ['random']#, 'smotenc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216115dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "over_threshold_choices = [float(f) for f in np.linspace(0, 4, num=17).round(2)] + ['auto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace42fb-1289-489a-9223-d755eb5406d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "\n",
    "def build_oversampling_strategy(value_counts, threshold):\n",
    "    n_occurrences = sum(value_counts.values())\n",
    "    perfectly_balanced_occurrences = int(n_occurrences / len(value_counts.keys()))\n",
    "\n",
    "    if threshold == \"auto\":\n",
    "        n_generate = {\n",
    "            class_: perfectly_balanced_occurrences - occ\n",
    "                    if occ < perfectly_balanced_occurrences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    else:\n",
    "        n_generate = {\n",
    "            class_: int(min(occ * threshold, perfectly_balanced_occurrences - occ))\n",
    "                    if occ < perfectly_balanced_occurrences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    return n_generate\n",
    "\n",
    "def patch_oversampling_strategy(value_counts, n_generate):\n",
    "    return {k: (value_counts[k] + n_generate[k]) for k in value_counts.keys()}\n",
    "\n",
    "def make_over_sampler(over_method, over_strategy, n_neighbors, cat_features=None):\n",
    "    if over_method == \"random\":\n",
    "        return CumlRandomResampler(strategy=over_strategy, sampling_type=\"over\")\n",
    "    elif over_method == \"smote\":\n",
    "        return SMOTE(k_neighbors=CumlNearestNeighbors(n_neighbors=n_neighbors), sampling_strategy=over_strategy)\n",
    "    elif over_method == \"smotenc\":\n",
    "        return SMOTENC(categorical_features=cat_features, k_neighbors=CumlNearestNeighbors(n_neighbors=n_neighbors), sampling_strategy=over_strategy)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown oversampling method: {over_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917b490-3e36-4256-bbf0-961a507c5307",
   "metadata": {},
   "source": [
    "### Step 7.3b: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e61da",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "under_method_choices = ['random']#, 'tomek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0b253",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "under_threshold_choices = [float(f) for f in np.linspace(0, 0.95, num=20).round(2)] + ['auto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc59bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks as TomekLinksImblearn\n",
    "from sklearn.utils import _safe_indexing  # Needed for compatibility\n",
    "\n",
    "class TomekLinksCUDA(TomekLinksImblearn):\n",
    "    def fit_resample(self, X, y):\n",
    "        nn = CumlNearestNeighbors(n_neighbors=2)\n",
    "        nn.fit(X)\n",
    "        nns = nn.kneighbors(X, return_distance=False)[:, 1]\n",
    "\n",
    "        links = self.is_tomek(y, nns, self.sampling_strategy_)\n",
    "        self.sample_indices_ = np.flatnonzero(np.logical_not(links))\n",
    "\n",
    "        return (\n",
    "            _safe_indexing(X, self.sample_indices_),\n",
    "            _safe_indexing(y, self.sample_indices_),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0899ed9-91b9-48e7-99ff-1635ffd3d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def build_undersampling_strategy(value_counts, threshold):\n",
    "    n_occurences = sum([n for n in value_counts.values()])\n",
    "    perfectly_balanced_occurences = int(n_occurences / len(value_counts.keys()))\n",
    "    if threshold == \"auto\":\n",
    "        n_remove = {\n",
    "            class_: occ - perfectly_balanced_occurences\n",
    "                    if occ > perfectly_balanced_occurences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    else:\n",
    "        n_remove = {\n",
    "            class_: int(min(occ * threshold, occ - perfectly_balanced_occurences))\n",
    "            if occ > perfectly_balanced_occurences else 0\n",
    "            for class_, occ in value_counts.items()\n",
    "        }\n",
    "    return n_remove\n",
    "\n",
    "def patch_undersampling_strategy(value_counts, n_remove):\n",
    "    return {k : (value_counts[k] - n_remove[k]) for k in value_counts.keys()}\n",
    "\n",
    "def make_under_sampler(under_method, under_strategy):\n",
    "    if under_method == \"random\":\n",
    "        return CumlRandomResampler(strategy=under_strategy, sampling_type=\"under\")\n",
    "    elif under_method == \"tomek\":\n",
    "        return TomekLinksCUDA(n_jobs=n_jobs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown undersampling method: {under_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36776a83-f2ec-47a5-b036-b79ce314593c",
   "metadata": {},
   "source": [
    "## Step 7.4: Study Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06241f6-6166-4db9-ba53-2d31dc99530d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "y_train_filtered = cudf.Series(y_train_filtered)\n",
    "\n",
    "if y_train_filtered.value_counts().min() < min_samples_per_class:\n",
    "    df_temp = cudf.concat([\n",
    "        X_train_filtered.reset_index(drop=True),\n",
    "        y_train_filtered.reset_index(drop=True)\n",
    "        ], axis='columns')\n",
    "    df_temp = ensure_min_samples_per_class(df_temp, target_column, min_samples_per_class, random_state)\n",
    "    X_train_filtered, y_train_filtered = df_temp.drop(columns=[target_column]), df_temp[target_column]\n",
    "    assert y_train_filtered.value_counts().min() >= min_samples_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd1c98-6305-4084-bbb8-cebc37266a0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "assert isinstance(X_train_filtered, cudf.DataFrame)\n",
    "assert isinstance(y_train_filtered, cudf.Series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c78e02-4c79-447a-a234-3f5cd97f19d9",
   "metadata": {},
   "source": [
    "## Step 7.5: Study Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bc82a-8078-4ba9-8fd4-07f360233421",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "This part is skipped because the pareto-front configs will be refitted based on prevous HPO logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c1df2-71e6-45cf-a67c-6a8f30bc193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "hyperparam_keys = [\n",
    "    'balancing_mode', 'over_method', 'over_threshold',\n",
    "    'under_method', 'under_threshold', 'eta', 'max_depth',\n",
    "    'min_child_weight', 'subsample', 'colsample_bytree',\n",
    "    'lambda', 'alpha'\n",
    "]\n",
    "\n",
    "def is_pareto_efficient(df, score_col='f1_score', latency_col='latency'):\n",
    "    data = df[[score_col, latency_col]].values\n",
    "    is_efficient = np.ones(data.shape[0], dtype=bool)\n",
    "    for i, (score, latency) in enumerate(data):\n",
    "        if is_efficient[i]:\n",
    "            is_dominated = (data[:, 0] >= score) & (data[:, 1] <= latency)\n",
    "            is_dominated[i] = False\n",
    "            is_efficient[is_dominated] = False\n",
    "    return is_efficient\n",
    "\n",
    "dataset_path_refit = dataset_path.replace('/Input_Multiclass/', '/Output_Multiclass_3600/').replace('.parquet', '/')\n",
    "\n",
    "print(dataset_path_refit)\n",
    "\n",
    "hpo_configs_to_refit = []\n",
    "\n",
    "try:\n",
    "    \n",
    "    hpo_trials_df_filename = os.path.join(dataset_path_refit, 'xgb_hpo_trials.xlsx')\n",
    "\n",
    "    print(hpo_trials_df_filename)\n",
    "    \n",
    "    hpo_trials_df = pd.read_excel(hpo_trials_df_filename)\n",
    "    \n",
    "    for col in hpo_trials_df.columns:\n",
    "        try:\n",
    "            # Try to replace ',' with '.' and convert to float\n",
    "            if col in ['max_depth', 'min_child_weight']:\n",
    "                hpo_trials_df[col] = hpo_trials_df[col].astype(int)\n",
    "            else:\n",
    "                hpo_trials_df[col] = hpo_trials_df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "        except ValueError:\n",
    "            # If conversion fails (e.g., for categorical strings), skip the column\n",
    "            continue\n",
    "    \n",
    "    # Get boolean mask for Pareto front\n",
    "    hpo_pareto_mask = is_pareto_efficient(hpo_trials_df)\n",
    "    \n",
    "    # Extract Pareto-optimal rows\n",
    "    hpo_pareto_df = hpo_trials_df[hpo_pareto_mask].reset_index(drop=True)\n",
    "   \n",
    "    hpo_configs_to_refit = hpo_pareto_df[hyperparam_keys].to_dict(orient='records')\n",
    "\n",
    "    print(hpo_trials_df_filename, type(hpo_configs_to_refit), len(hpo_configs_to_refit))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error processing folder {hpo_trials_df_filename}. Reason: {str(e)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c31a76-d6d9-430b-b3a3-c6bda98d60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(hpo_configs_to_refit, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d22ac0-bd55-4c3f-b5a2-63126fc74803",
   "metadata": {},
   "source": [
    "# Step 8: Pipeline Reconstruction and Refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a79eac-d364-47c5-9ad1-9193b62b3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def reconstruct_and_evaluate_pareto_model(best_params, pareto_idx):\n",
    "    # Deep copy to avoid modifying original dict\n",
    "    best_params_copy = deepcopy(best_params)\n",
    "\n",
    "    # Extract balancing strategy\n",
    "    balancing_mode = best_params_copy.pop(\"balancing_mode\")\n",
    "    sample_weights = None  # Default is no weighting\n",
    "\n",
    "    # Handle each balancing mode\n",
    "\n",
    "    if balancing_mode == \"none\":\n",
    "        # No resampling or weighting\n",
    "        X_train_final, y_train_final = X_train_filtered, y_train_filtered\n",
    "    \n",
    "    elif balancing_mode == \"resampling\":\n",
    "        # Extract resampling params\n",
    "        best_over_method = best_params_copy.pop(\"over_method\")\n",
    "        best_over_threshold = best_params_copy.pop(\"over_threshold\")\n",
    "        best_under_method = best_params_copy.pop(\"under_method\")\n",
    "        best_under_threshold = best_params_copy.pop(\"under_threshold\")\n",
    "\n",
    "        # Perform resampling\n",
    "        X_train_final, y_train_final = fit_resample(\n",
    "            X_train_filtered, y_train_filtered,\n",
    "            best_over_method, best_over_threshold,\n",
    "            best_under_method, best_under_threshold\n",
    "        )\n",
    "\n",
    "    elif balancing_mode == \"weighting\":\n",
    "        # No resampling; just apply inverse frequency weighting\n",
    "        X_train_final, y_train_final = X_train_filtered, y_train_filtered\n",
    "\n",
    "        class_counts = y_train_final.value_counts()\n",
    "        weight_map = 1.0 / class_counts\n",
    "        sample_weights = y_train_final.map(weight_map)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid balancing_mode: {balancing_mode}\")\n",
    "\n",
    "    # Train final model on full training set\n",
    "    model_refit, train_time_refit, latency_refit, f1_weighted_refit, model_size_refit = train_xgb(\n",
    "        X_train_final, X_val_reduced, X_test_reduced,\n",
    "        y_train_final, y_val_sampled, y_test_sampled,\n",
    "        sample_weights=sample_weights,\n",
    "        cv=False,  # Final training, no CV\n",
    "        custom_booster_params=best_params_copy,\n",
    "        metrics_filename=f\"{output_folder}/pareto_trials/xgb_hpo_pareto_{pareto_idx}_metrics.json\",\n",
    "        cm_filename=f\"{output_folder}/pareto_trials/xgb_hpo_pareto_{pareto_idx}_cm.csv\",\n",
    "        model_filename=f\"{output_folder}/pareto_trials/xgb_hpo_pareto_{pareto_idx}_model.json\"\n",
    "    )\n",
    "\n",
    "    # Report\n",
    "    print(f\"Pareto Index = {pareto_idx} => Training Time: {train_time_refit:.3f} seconds\")\n",
    "    print(f\"Pareto Index = {pareto_idx} => Latency: {latency_refit:.2e} seconds\")\n",
    "    print(f\"Pareto Index = {pareto_idx} => Weighted F1-Score: {f1_weighted_refit:.6f}\")\n",
    "    print(f\"Pareto Index = {pareto_idx} => Model Size: {model_size_refit} MB\")\n",
    "\n",
    "    return X_train_final, model_size_refit, train_time_refit, latency_refit, f1_weighted_refit, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6cd17-b5b5-492d-a868-570b33131b7c",
   "metadata": {},
   "source": [
    "## Step 9: Pareto Front Evaluations (with Sampling, Feature Selection, Row Filtering, and HPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be204c-ad95-4ec7-b66b-85a75cfbd55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_trials = []\n",
    "\n",
    "for pareto_idx, curr_params in enumerate(hpo_configs_to_refit):\n",
    "\n",
    "    pareto_idx_str = str(pareto_idx).zfill(len(str(len(hpo_configs_to_refit))))\n",
    "    \n",
    "    print(pareto_idx_str, curr_params)\n",
    "    \n",
    "    X_train_resampled_refit, model_size_refit, train_time_refit, latency_refit, f1_weighted_refit, best_params_refit = \\\n",
    "        reconstruct_and_evaluate_pareto_model(curr_params, pareto_idx_str)\n",
    "    \n",
    "    refit_trials.append({\n",
    "        'pareto_index': pareto_idx,\n",
    "        'params': curr_params,\n",
    "        'results': {\n",
    "            'model_size': model_size_refit,\n",
    "            'training_time': train_time_refit,\n",
    "            'latency': latency_refit,\n",
    "            'f1_weighted': f1_weighted_refit\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    if pareto_idx == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d6335-41fe-457f-938a-43b3655dd394",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_stats_df = pd.DataFrame.from_dict({x['pareto_index']: x['results'] for x in refit_trials}, orient='index')\n",
    "\n",
    "refit_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8d8796-c519-40b1-8cfb-4d31458b9aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_stats = refit_stats_df.agg(['min', 'max', 'mean', 'std']).to_dict()\n",
    "\n",
    "pprint(refit_stats, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c84883-c0a2-4674-aec8-5344861aca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "refit_results_filename = os.path.join(output_folder, 'pareto_trials', f'summary.json')\n",
    "\n",
    "os.makedirs(Path(refit_results_filename).parent, exist_ok=True)\n",
    "\n",
    "refit_dict = {\n",
    "    'trials': refit_trials,\n",
    "    'stats': refit_stats\n",
    "}\n",
    "\n",
    "pprint(refit_dict, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381c2fa1-818d-45b3-b620-44e49b6e7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(refit_results_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(refit_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0931325",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Global Time: {time.time() - global_start:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

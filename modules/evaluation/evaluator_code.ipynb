{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0dbe0",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_path = '../../2025-07-05/Input_Multiclass/CICAPT_IIoT_Phase1_Micro_Multiclass.parquet'\n",
    "output_folder = \"../../2025-07-05/Output_Multiclass_Fixes_Temp\"\n",
    "target_column = 'label'\n",
    "handle_object_cols = 'keep'\n",
    "sampling_rate_global = None # 0.10\n",
    "sampling_rate_sets = 0.10\n",
    "sample_sets = ['train']\n",
    "min_samples_per_class = 1\n",
    "feature_selection_threshold = 0.99\n",
    "sample_filtering_quantile = 0.10\n",
    "hpo_n_trials = 10 # 1000\n",
    "hpo_timeout = 60 # 3600\n",
    "num_boost_round = 100 # 500\n",
    "early_stopping_rounds = 10 # 50\n",
    "n_jobs = -1\n",
    "random_state = 42\n",
    "plot_param_importances = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8bcd5",
   "metadata": {
    "editable": true,
    "lines_to_next_cell": 2,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optuna.exceptions import ExperimentalWarning\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa25f30-c577-4bc0-ace4-d8cf91d15b18",
   "metadata": {},
   "source": [
    "# Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a877e4-d4e2-408f-9ec8-7acf40ea9cf0",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "random.seed(random_state)\n",
    "cp.random.seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "global_start = time.time()\n",
    "\n",
    "# Step 1: read dataset from parquet file\n",
    "df_full = cudf.read_parquet(dataset_path)\n",
    "\n",
    "col_info = {str(col): {} for col in df_full.columns}\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b639161-1f7f-40c2-b994-f6e55f5f9895",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8b4fc2-bffa-4996-96e6-651e8b238a63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 2: sample dataset if specified\n",
    "if sampling_rate_global:\n",
    "    df_full = df_full.sample(frac=sampling_rate_global, random_state=random_state)\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_1': df_full[col].nunique(), 'dtype_1': df_full[col].dtype, 'mem_1': df_full[col].memory_usage() / 1024**2})\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b777a-a275-4648-9da8-b7aeda1d349a",
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 3: drop object columns if specified\n",
    "if handle_object_cols != 'keep':\n",
    "    object_cols = df_full.drop(columns=[target_column]).select_dtypes(include='object').columns    \n",
    "    if handle_object_cols == 'drop' and len(object_cols) > 0:\n",
    "        df_full = df_full.drop(columns=object_cols)\n",
    "    elif handle_object_cols == 'encode' and len(object_cols) > 0:\n",
    "        for col in object_cols:\n",
    "            n_unique = df_full[col].nunique()\n",
    "            if n_unique <= 1000:\n",
    "                df_full[col] = df_full[col].astype('category')\n",
    "            else:\n",
    "                col_str = df_full[col].astype(str)\n",
    "                hashed = col_str.hash_values(seed=seed).astype('int64') + seed\n",
    "                df_full[col] = (hashed % 1024).astype('int16')\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_2': df_full[col].nunique(), 'dtype_2': df_full[col].dtype, 'mem_2': df_full[col].memory_usage() / 1024**2})\n",
    "\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d2aa1-5f4b-4237-87b7-4dd2477579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(dataset_path.replace('.parquet', '.json'), 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Step 4: restore dtypes for other columns from metadata\n",
    "for col, dtype in metadata[\"dtypes\"].items():\n",
    "    if col in df_full.columns:# and col not in object_cols:\n",
    "        df_full[col] = df_full[col].astype(dtype)\n",
    "\n",
    "for col in df_full.columns:\n",
    "    col_info[str(col)].update({'nunique_3': df_full[col].nunique(), 'dtype_3': df_full[col].dtype, 'mem_3': df_full[col].memory_usage() / 1024**2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a741d-f8b2-434c-b6d7-ee68c06a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(col_info).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3736105-f7b1-4b57-a1b8-a9d20016bb15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_full.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5ded8-a1ce-4547-a46c-c2173f4d214e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_full[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5e122",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Factorize target column\n",
    "df_full[target_column], unique_values = df_full[target_column].factorize()\n",
    "assert df_full[target_column].nunique() >= 2, \"Classification requires two or more classes.\"\n",
    "label_to_index = {value: i for i, value in enumerate(unique_values.to_pandas())}\n",
    "index_to_label = {v: k for k, v in label_to_index.items()}\n",
    "labels = list(label_to_index.keys())\n",
    "df_full[target_column] = df_full[target_column].astype('int8')\n",
    "\n",
    "# Detect and assign codes to categorical columns\n",
    "numeric_columns = df_full.drop(columns=[target_column]).select_dtypes(include=['number']).columns.tolist()\n",
    "categorical_cols = df_full.drop(columns=[target_column]).select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "df_full[target_column].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.model_selection import train_test_split\n",
    "\n",
    "def ensure_min_samples_per_class(df, stratify_col, min_samples_per_class, random_state):\n",
    "    \"\"\"Ensures that each class has at least `min_samples_per_class` samples using oversampling.\"\"\"\n",
    "    class_counts = df[stratify_col].value_counts().to_pandas()\n",
    "\n",
    "    # Oversample minority classes\n",
    "    oversampled = [\n",
    "        df[df[stratify_col] == c].sample(n=min_samples_per_class, replace=True, random_state=random_state)\n",
    "        for c in class_counts[class_counts < min_samples_per_class].index\n",
    "    ]\n",
    "    df_oversampled = cudf.concat([df] + oversampled, ignore_index=True) if oversampled else df\n",
    "\n",
    "    return df_oversampled.reset_index(drop=True)  # Fix index mismatch\n",
    "\n",
    "def restore_dtypes(df, dtypes):\n",
    "    for col, dtype in dtypes.to_dict().items():\n",
    "        df[col] = df[col].astype(dtype)\n",
    "\n",
    "def assign_subsets(df, stratify_col, train_frac, val_frac, test_frac, random_state):\n",
    "    \"\"\"Splits data into mutually exclusive train/val/test subsets before applying stratified sampling.\"\"\"\n",
    "    assert train_frac + val_frac + test_frac == 1.0, \"Fractions must sum to 1\"\n",
    "\n",
    "    # Assign subset labels\n",
    "    df_train, df_temp = train_test_split(df, test_size=(1 - train_frac), stratify=df[stratify_col], random_state=random_state)\n",
    "    df_val, df_test = train_test_split(df_temp, test_size=(test_frac / (val_frac + test_frac)), stratify=df_temp[stratify_col], random_state=random_state)\n",
    "\n",
    "    df_train[\"subset\"] = \"train\"\n",
    "    df_val[\"subset\"] = \"val\"\n",
    "    df_test[\"subset\"] = \"test\"\n",
    "\n",
    "    return cudf.concat([df_train, df_val, df_test]).reset_index(drop=True)\n",
    "\n",
    "def sample_group(x):\n",
    "    \"\"\"Helper function to stratify sample while ensuring class presence.\"\"\"\n",
    "    n_samples = max(min_samples_per_class, int(len(x) * sampling_rate_sets))\n",
    "    return x.sample(n=n_samples, replace=len(x) < n_samples, random_state=random_state)\n",
    "\n",
    "def stratified_sample(df, stratify_col, sample_sets, sampling_rate_sets, min_samples_per_class, random_state):\n",
    "    \"\"\"Applies stratified sampling while ensuring minimum samples per class, only for selected subsets.\"\"\"\n",
    "\n",
    "    # Apply stratified sampling only for the requested subsets\n",
    "    df_sampled = df.groupby([\"subset\", stratify_col], group_keys=False).apply(\n",
    "        lambda x: sample_group(x) if x[\"subset\"].iloc[0] in sample_sets else x\n",
    "    )\n",
    "\n",
    "    return df_sampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3affed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dtypes_before = df_full.dtypes.copy(deep=True).sort_index()\n",
    "\n",
    "# Ensure enough samples before splitting\n",
    "split_ok = False\n",
    "while not split_ok:\n",
    "    df_full = ensure_min_samples_per_class(df_full, target_column, min_samples_per_class, random_state)\n",
    "    df_full = df_full.reset_index(drop=True)  # Reset index before splitting\n",
    "    df_dtypes_backup = df_full.dtypes.copy(deep=True)\n",
    "\n",
    "    # Factorize for stratification\n",
    "    category_mappings = {}\n",
    "\n",
    "    for col, dtype in df_full.dtypes.to_dict().items():\n",
    "        if dtype == 'category':\n",
    "            codes, uniques = df_full[col].factorize()\n",
    "            df_full[col] = codes.astype('int32')\n",
    "            category_mappings[col] = uniques.to_pandas().tolist()\n",
    "\n",
    "    try:\n",
    "        # Assign mutually exclusive train/val/test subsets\n",
    "        df_full = assign_subsets(df_full, \"label\", train_frac=0.6, val_frac=0.2, test_frac=0.2, random_state=random_state)\n",
    "        split_ok = True  # If it succeeds, exit loop\n",
    "    except ValueError as e:\n",
    "        print(f\"Resampling due to insufficient class representation (min_samples_per_class={min_samples_per_class})...\")\n",
    "        min_samples_per_class += 1  # Increment dynamically and retry\n",
    "    \n",
    "    for col, dtype in df_dtypes_backup.items():\n",
    "        if col in category_mappings:\n",
    "            # Restore category values from factorized codes\n",
    "            mapping = dict(enumerate(category_mappings[col]))\n",
    "            df_full[col] = df_full[col].map(mapping).astype('category')\n",
    "        else:\n",
    "            # Restore other dtypes (numeric, bool, etc.)\n",
    "            df_full[col] = df_full[col].astype(dtype)\n",
    "\n",
    "print(f\"Minimal oversampling completed successfully (min_samples_per_class={min_samples_per_class}).\")\n",
    "\n",
    "df_full[target_column].value_counts()\n",
    "\n",
    "df_dtypes_after = df_full.dtypes.copy(deep=True).drop('subset').sort_index()\n",
    "\n",
    "assert df_dtypes_before.equals(df_dtypes_after), \"Dtype mismatch after assigning subsets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4bd60-b3fd-4776-a522-5ab06e58bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing categorical values\n",
    "def fill_categorical_nas(dfs):\n",
    "    return\n",
    "    for df in dfs:\n",
    "        for col in df.select_dtypes(include=['category']).columns:\n",
    "            if df[col].to_pandas().isna().sum() > 0:\n",
    "                df[col] = cudf.Series(df[col].to_pandas().astype(str).replace(\"nan\", \"missing\").astype(\"category\"))\n",
    "\n",
    "# Extract final datasets\n",
    "df_train_full = df_full[df_full[\"subset\"] == \"train\"].drop(columns=[\"subset\"])\n",
    "df_val_full = df_full[df_full[\"subset\"] == \"val\"].drop(columns=[\"subset\"])\n",
    "df_test_full = df_full[df_full[\"subset\"] == \"test\"].drop(columns=[\"subset\"])\n",
    "\n",
    "# Convert back to X, y format\n",
    "X_train_full, y_train_full = df_train_full.drop(columns=[target_column]), df_train_full[target_column]\n",
    "X_val_full, y_val_full = df_val_full.drop(columns=[target_column]), df_val_full[target_column]\n",
    "X_test_full, y_test_full = df_test_full.drop(columns=[target_column]), df_test_full[target_column]\n",
    "\n",
    "fill_categorical_nas([X_train_full, X_val_full, X_test_full])\n",
    "\n",
    "# Ensure disjoint splits\n",
    "assert set(X_train_full.to_pandas().index).isdisjoint(set(X_val_full.to_pandas().index)), \"Train and Validation sets are not disjoint!\"\n",
    "assert set(X_train_full.to_pandas().index).isdisjoint(set(X_test_full.to_pandas().index)), \"Train and Test sets are not disjoint!\"\n",
    "assert set(X_val_full.to_pandas().index).isdisjoint(set(X_test_full.to_pandas().index)), \"Validation and Test sets are not disjoint!\"\n",
    "\n",
    "# Ensure all classes are present in each split\n",
    "assert y_train_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Train!\"\n",
    "assert y_val_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Validation!\"\n",
    "assert y_test_full.nunique() == df_full[target_column].nunique(), \"Some classes are missing in Test!\"\n",
    "\n",
    "# Print final distributions\n",
    "print(f\"Train      : {len(df_train_full)} samples ({(100.0 * len(df_train_full) / len(df_full)):.2f}%), {y_train_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")\n",
    "print(f\"Validation : {len(df_val_full)} samples ({(100.0 * len(df_val_full) / len(df_full)):.2f}%), {y_val_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")\n",
    "print(f\"Test       : {len(df_test_full)} samples ({(100.0 * len(df_test_full) / len(df_full)):.2f}%), {y_test_full.nunique()} unique classes ({sorted(y_train_full.to_pandas().unique().tolist())})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b17bb2a-7390-4246-870c-0f8425a01a7f",
   "metadata": {},
   "source": [
    "# Step 2: Model and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea6e59",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score,\n",
    "    precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, cohen_kappa_score,\n",
    "    log_loss, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "num_classes = df_full[target_column].nunique()\n",
    "loss_function = 'binary:logistic' if num_classes == 2 else 'multi:softprob'\n",
    "eval_metric = 'logloss' if num_classes == 2 else 'mlogloss'\n",
    "n_folds, shuffle, stratify = 5, True, True\n",
    "\n",
    "def evaluate_multiclass_metrics(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    y_prob=None,\n",
    "    labels=None,\n",
    "    label_names=None\n",
    "):\n",
    "    metrics = {}\n",
    "\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"balanced_accuracy\"] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
    "    metrics[\"cohen_kappa\"] = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "    for avg in [\"micro\", \"macro\", \"weighted\"]:\n",
    "        metrics[f\"precision_{avg}\"] = precision_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "        metrics[f\"recall_{avg}\"] = recall_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "        metrics[f\"f1_{avg}\"] = f1_score(y_true, y_pred, average=avg, zero_division=0)\n",
    "\n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            metrics[\"log_loss\"] = log_loss(y_true, y_prob, labels=labels)\n",
    "        except:\n",
    "            metrics[\"log_loss\"] = np.nan\n",
    "\n",
    "    if y_prob is not None and labels is not None and len(np.unique(y_true)) > 1:\n",
    "        for avg in [\"micro\", \"macro\", \"weighted\"]:\n",
    "            try:\n",
    "                metrics[f\"roc_auc_ovr_{avg}\"] = roc_auc_score(y_true, y_prob, multi_class=\"ovr\", average=avg, labels=labels)\n",
    "                metrics[f\"roc_auc_ovo_{avg}\"] = roc_auc_score(y_true, y_prob, multi_class=\"ovo\", average=avg, labels=labels)\n",
    "            except:\n",
    "                metrics[f\"roc_auc_ovr_{avg}\"] = np.nan\n",
    "                metrics[f\"roc_auc_ovo_{avg}\"] = np.nan\n",
    "\n",
    "    if labels is None:\n",
    "        labels = np.unique(y_true)\n",
    "    conf = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    display_labels = label_names if label_names is not None else labels\n",
    "    conf_df = pd.DataFrame(conf, index=[f\"True_{l}\" for l in display_labels],\n",
    "                                 columns=[f\"Pred_{l}\" for l in display_labels])\n",
    "\n",
    "    return metrics, conf_df\n",
    "\n",
    "def f1_weighted_eval(preds, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    weights = dtrain.get_weight()\n",
    "    # Convert probabilities to class labels\n",
    "    if num_classes > 2:\n",
    "        y_pred = np.argmax(preds.reshape(y_true.shape[0], -1), axis=1)\n",
    "    else:\n",
    "        y_pred = (preds > 0.5).astype(int)\n",
    "    # Use weights if provided\n",
    "    if weights is not None and len(weights) > 0:\n",
    "        f1 = f1_score(y_true, y_pred, average=\"weighted\", sample_weight=weights)\n",
    "    else:\n",
    "        f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    return \"f1_weighted\", f1\n",
    "\n",
    "def get_model_size(model):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".ubj\") as temp_model:\n",
    "        model_path = temp_model.name\n",
    "    model.save_model(model_path)\n",
    "    model_size_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    os.remove(model_path)\n",
    "    return round(model_size_mb, 2)\n",
    "\n",
    "# Default XGB Booster parameters\n",
    "default_booster_params = {\n",
    "    \"objective\": loss_function,                     # Multi-class classification\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of iterations\n",
    "    \"num_boost_round\": num_boost_round,             # Number of CV folds\n",
    "    \"eval_metric\": eval_metric,                     # Log loss\n",
    "    \"device\": 'cuda'                                # GPU (CUDA)\n",
    "}\n",
    "if num_classes > 2:\n",
    "    default_booster_params[\"num_class\"] = num_classes # Only for multi-class classification\n",
    "\n",
    "# Default XGB CV parameters\n",
    "default_cv_params = {\n",
    "    \"params\": default_booster_params,               # Booster parameters\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of rounds before early stopping\n",
    "    \"num_boost_round\": num_boost_round,             # Number of iterations\n",
    "    \"nfold\": n_folds,                               # Number of CV folds\n",
    "    \"shuffle\": shuffle,                             # Shuffle samples before creating folds\n",
    "    \"stratified\": stratify,                         # Stratify classes on CV split\n",
    "    \"metrics\": eval_metric,                         # Log loss\n",
    "    \"feval\": f1_weighted_eval,                      # Monitor weighted F1 score\n",
    "    \"seed\": random_state                            # Passed to numpy.random.seed\n",
    "}\n",
    "\n",
    "# Default XGB training parameters\n",
    "default_train_params = {\n",
    "    \"early_stopping_rounds\": early_stopping_rounds, # Number of rounds before early stopping\n",
    "    \"num_boost_round\": num_boost_round,             # Number of iterations\n",
    "}\n",
    "\n",
    "# Function to Train, Validate, and Test XGBoost Model\n",
    "def train_xgb(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    sample_weights=None,\n",
    "    cv=False,\n",
    "    custom_booster_params=None,\n",
    "    metrics_filename: str = None,\n",
    "    cm_filename: str = None,\n",
    "    model_filename: str = None,\n",
    "    verbose=1\n",
    "):\n",
    "    # Warmup verification\n",
    "    assert sample_weights is None or len(sample_weights) == len(y_train), \"Sample weights must align with training labels\"\n",
    "\n",
    "    # Create DMatrix with feature names\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True, feature_names=X_train.columns.tolist(), weight=sample_weights)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val, enable_categorical=True, feature_names=X_val.columns.tolist()) if X_val is not None else None\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True, feature_names=X_test.columns.tolist()) if X_test is not None else None\n",
    "\n",
    "    # Merge default booster params with custom ones\n",
    "    booster_params = {**default_booster_params, **(custom_booster_params or {})}\n",
    "\n",
    "    if cv:\n",
    "        cv_params = {\n",
    "            **default_cv_params,\n",
    "            \"params\": booster_params,\n",
    "            \"verbose_eval\": verbose\n",
    "        }\n",
    "        training_start = time.time()\n",
    "        results = xgb.cv(dtrain=dtrain, **cv_params)\n",
    "        training_end = time.time()\n",
    "        training_time = (training_end - training_start) / n_folds\n",
    "        latency = training_time / len(y_train)\n",
    "        f1_score_ans = float(results['test-f1_weighted-mean'].mean())\n",
    "        model, model_size = None, None\n",
    "\n",
    "        # Persist the model properly\n",
    "        if metrics_filename:\n",
    "            os.makedirs(os.path.dirname(metrics_filename), exist_ok=True)\n",
    "            results.to_csv(metrics_filename, index=True)\n",
    "\n",
    "    else:\n",
    "        training_start = time.time()\n",
    "        model = xgb.train(\n",
    "            params=booster_params,\n",
    "            dtrain=dtrain,\n",
    "            evals=[(dtrain, \"Train\"), (dval, \"Validation\")],\n",
    "            **default_train_params\n",
    "        )\n",
    "        training_end = time.time()\n",
    "        training_time = training_end - training_start\n",
    "\n",
    "        # Predict\n",
    "        test_start = time.time()\n",
    "        y_pred_probs = model.predict(dtest)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1) if num_classes > 2 else (y_pred_probs > 0.5).astype(int)\n",
    "        test_end = time.time()\n",
    "        test_time = test_end - test_start\n",
    "\n",
    "        latency = test_time / len(y_test)\n",
    "\n",
    "        model_size = get_model_size(model)\n",
    "\n",
    "        metrics_dict, confusion_df = evaluate_multiclass_metrics(\n",
    "            y_true=y_test.to_numpy(),\n",
    "            y_pred=y_pred,\n",
    "            y_prob=y_pred_probs,\n",
    "            labels=list(index_to_label.keys()),\n",
    "            label_names=[index_to_label[i] for i in index_to_label]\n",
    "        )\n",
    "        metrics_dict.update({\n",
    "            'training_time': training_time, 'test_time': test_time, 'latency': latency, 'model_size': model_size\n",
    "        })\n",
    "        \n",
    "        f1_score_ans = metrics_dict['f1_weighted']\n",
    "\n",
    "        # Persist the results properly\n",
    "        if metrics_filename:\n",
    "            os.makedirs(os.path.dirname(metrics_filename), exist_ok=True)\n",
    "            with open(metrics_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(metrics_dict, f, indent=2)\n",
    "        if cm_filename:\n",
    "            os.makedirs(os.path.dirname(cm_filename), exist_ok=True)\n",
    "            confusion_df.to_csv(cm_filename, index=True)\n",
    "        if model_filename:\n",
    "            os.makedirs(os.path.dirname(model_filename), exist_ok=True)\n",
    "            model.save_model(model_filename)\n",
    "\n",
    "    return model, training_time, latency, f1_score_ans, model_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec7c3f2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# train_xgb(X_train_full, None, None, y_train_full, None, None, cv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ffa1a-e27a-4032-8231-9efd4aaf5ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgb(X_train_full, X_val_full, X_test_full, y_train_full, y_val_full, y_test_full, cv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6398b-ffe3-44ca-aa8b-6a6112fbfbc5",
   "metadata": {},
   "source": [
    "# Step 3: Baseline Evaluation (full dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a14a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "model_full, train_time_full, latency_full, f1_weighted_full, model_size_full = train_xgb(\n",
    "    X_train_full, X_val_full, X_test_full,\n",
    "    y_train_full, y_val_full, y_test_full,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_full_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_full_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_full_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (full): {train_time_full:.3f} seconds\")\n",
    "print(f\"Latency (full): {latency_full:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (full): {f1_weighted_full:.6f}\")\n",
    "print(f\"Model Size: {model_size_full} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b7b4c-5b79-46cc-8881-9cf6c8b25ed5",
   "metadata": {},
   "source": [
    "# Step 4: Updated Evaluation (with Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc95fe7-1d08-4874-8565-7b132525a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stratified sampling only to the selected subsets\n",
    "df_sampled = stratified_sample(df_full, target_column, sample_sets, sampling_rate_sets, min_samples_per_class, random_state)\n",
    "\n",
    "# Extract final datasets\n",
    "df_train_sampled = df_sampled[df_sampled[\"subset\"] == \"train\"].drop(columns=[\"subset\"])\n",
    "df_val_sampled = df_sampled[df_sampled[\"subset\"] == \"val\"].drop(columns=[\"subset\"])\n",
    "df_test_sampled = df_sampled[df_sampled[\"subset\"] == \"test\"].drop(columns=[\"subset\"])\n",
    "\n",
    "# Convert back to X, y format\n",
    "X_train_sampled, y_train_sampled = df_train_sampled.drop(columns=[target_column]), df_train_sampled[target_column]\n",
    "X_val_sampled, y_val_sampled = df_val_sampled.drop(columns=[target_column]), df_val_sampled[target_column]\n",
    "X_test_sampled, y_test_sampled = df_test_sampled.drop(columns=[target_column]), df_test_sampled[target_column]\n",
    "\n",
    "fill_categorical_nas([X_train_sampled, X_val_sampled, X_test_sampled])\n",
    "\n",
    "# Ensure disjoint splits\n",
    "assert set(X_train_sampled.to_pandas().index).isdisjoint(set(X_val_sampled.to_pandas().index)), \"Train and Validation sets are not disjoint!\"\n",
    "assert set(X_train_sampled.to_pandas().index).isdisjoint(set(X_test_sampled.to_pandas().index)), \"Train and Test sets are not disjoint!\"\n",
    "assert set(X_val_sampled.to_pandas().index).isdisjoint(set(X_test_sampled.to_pandas().index)), \"Validation and Test sets are not disjoint!\"\n",
    "\n",
    "# Ensure all classes are present in each split\n",
    "assert y_train_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Train!\"\n",
    "assert y_val_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Validation!\"\n",
    "assert y_test_sampled.nunique() == df_sampled[target_column].nunique(), \"Some classes are missing in Test!\"\n",
    "\n",
    "# Print final distributions\n",
    "print(f\"Train      : {len(df_train_sampled)} samples ({(100.0 * len(df_train_sampled) / len(df_full)):.2f}%), {y_train_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")\n",
    "print(f\"Validation : {len(df_val_sampled)} samples ({(100.0 * len(df_val_sampled) / len(df_full)):.2f}%), {y_val_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")\n",
    "print(f\"Test       : {len(df_test_sampled)} samples ({(100.0 * len(df_test_sampled) / len(df_full)):.2f}%), {y_test_sampled.nunique()} unique classes ({sorted(y_train_sampled.to_pandas().unique().tolist())})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea16e1b7-6372-45e6-81bc-94eb10754319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "model_sampled, train_time_sampled, latency_sampled, f1_weighted_sampled, model_size_sampled = train_xgb(\n",
    "    X_train_sampled, X_val_sampled, X_test_sampled,\n",
    "    y_train_sampled, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_sampled_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_sampled_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_sampled_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (sampled): {train_time_sampled:.3f} seconds\")\n",
    "print(f\"Latency (sampled): {latency_sampled:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (sampled): {f1_weighted_sampled:.6f}\")\n",
    "print(f\"Model Size: {model_size_sampled} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a82ddb-cb5c-42d4-b6b4-808f097c0c62",
   "metadata": {},
   "source": [
    "# Step 5: Updated Evaluation (with Sampling and Feature Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89e6ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate and sort features by importance\n",
    "feature_importance = model_sampled.get_score(importance_type=\"gain\")\n",
    "sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "features, importance = zip(*sorted_features)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "bars = plt.barh(features, importance, color='skyblue')\n",
    "\n",
    "# Annotate each bar with its importance value\n",
    "for bar, value in zip(bars, importance):\n",
    "    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\" {value:.3f}\", va='center')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"Gain (Importance)\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance (Gain)\")\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)  # Add grid for better readability\n",
    "\n",
    "plt.savefig(f\"{output_folder}/xgb_feature_importance_gain.pdf\", bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1c66cf-7f00-469a-b210-eb8f466fa1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = model_sampled.get_score(importance_type=\"gain\")\n",
    "importance_df = cudf.DataFrame(list(feature_importance.items()), columns=[\"Feature\", \"Importance\"])\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Normalize importance scores\n",
    "importance_df[\"Cumulative_Importance\"] = importance_df[\"Importance\"].cumsum() / importance_df[\"Importance\"].sum()\n",
    "\n",
    "# Find the smallest set of features that explains at least 95% of the importance\n",
    "N = np.argmax(importance_df[\"Cumulative_Importance\"] >= feature_selection_threshold) + 1  # Adjust threshold as needed\n",
    "\n",
    "# Select top features\n",
    "selected_features = importance_df[\"Feature\"][:N].to_pandas().tolist()\n",
    "\n",
    "print(f\"Optimal number of features: {N} (from {len(feature_importance)})\")\n",
    "\n",
    "importance_df.to_csv(f\"{output_folder}/xgb_feature_importance_gain.csv\", index=False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b8e11b-daee-4999-bfa7-6d83e6603780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the feature space for train, validation, and test sets\n",
    "X_train_reduced = X_train_sampled[selected_features]\n",
    "X_val_reduced = X_val_sampled[selected_features]\n",
    "X_test_reduced = X_test_sampled[selected_features]\n",
    "\n",
    "fill_categorical_nas([X_train_reduced, X_val_reduced, X_test_reduced])\n",
    "\n",
    "# Train and evaluate using train_xgb function\n",
    "model_reduced, train_time_reduced, latency_reduced, f1_weighted_reduced, model_size_reduced = train_xgb(\n",
    "    X_train_reduced, X_val_reduced, X_test_reduced,\n",
    "    y_train_sampled, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_reduced_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_reduced_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_reduced_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (reduced): {train_time_reduced:.3f} seconds\")\n",
    "print(f\"Latency (reduced): {latency_reduced:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (reduced): {f1_weighted_reduced:.6f}\")\n",
    "print(f\"Model Size (reduced): {model_size_reduced} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c15b394-b297-4511-88d8-4c77f78bc5bb",
   "metadata": {},
   "source": [
    "# Step 6: Updated Evaluation (with Sampling, Feature Selection, and Row Filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1424a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def convert_categorical_to_frequency(df, normalize=True):\n",
    "    \"\"\"Convert categorical features to frequency encoding with reversibility support.\"\"\"\n",
    "    df_encoded = df.copy()\n",
    "    category_mappings = {}\n",
    "    row_mappings = {}\n",
    "\n",
    "    for col in df_encoded.select_dtypes(include=['category']).columns:\n",
    "        # Calculate normalized frequencies\n",
    "        freqs = df[col].to_pandas().value_counts().astype('float32')\n",
    "        if normalize:\n",
    "            freqs = freqs / len(df)\n",
    "        freq_map = freqs.to_dict()\n",
    "\n",
    "        # Encode column\n",
    "        df_encoded[col] = df[col].to_pandas().map(freq_map).astype('float32')\n",
    "\n",
    "        # Save both mappings\n",
    "        category_mappings[col] = freq_map\n",
    "        row_mappings[col] = df[col].reset_index(drop=True)\n",
    "\n",
    "    return df_encoded, category_mappings, row_mappings\n",
    "\n",
    "\n",
    "def revert_frequency_encoding(df_encoded, row_mappings):\n",
    "    \"\"\"Revert frequency-encoded DataFrame to original categories using stored row-wise values.\"\"\"\n",
    "    df_reverted = df_encoded.copy().reset_index(drop=True)\n",
    "\n",
    "    for col in row_mappings:\n",
    "        if col in df_reverted.columns:\n",
    "            df_reverted[col] = row_mappings[col]  # restore from stored original\n",
    "\n",
    "    return df_reverted\n",
    "\n",
    "\n",
    "def filter_low_mean_samples(X_encoded: cudf.DataFrame, y: cudf.Series, quantile_threshold: float):\n",
    "    \"\"\"Filter low-mean samples per class while preserving all class labels.\"\"\"\n",
    "    \n",
    "    # Step 1: Combine features and label\n",
    "    combined_df = X_encoded.copy()\n",
    "    combined_df[target_column] = y\n",
    "\n",
    "    # Step 2: Compute row means once\n",
    "    row_means = X_encoded.mean(axis=1)\n",
    "    combined_df['row_mean'] = row_means\n",
    "\n",
    "    # Step 3: Compute quantile thresholds per class\n",
    "    class_thresholds = (\n",
    "        combined_df[[target_column, 'row_mean']]\n",
    "        .groupby(target_column)\n",
    "        .quantile(q=quantile_threshold)\n",
    "        .rename(columns={'row_mean': 'threshold'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Step 4: Join thresholds back to combined_df\n",
    "    combined_df = combined_df.merge(class_thresholds, on=target_column, how='left')\n",
    "\n",
    "    # Step 5: Apply filtering\n",
    "    filtered_df = combined_df[combined_df['row_mean'] > combined_df['threshold']]\n",
    "\n",
    "    # Step 6: Ensure all classes are represented\n",
    "    present_classes = set(cp.asnumpy(filtered_df[target_column].unique()))\n",
    "    all_classes = set(cp.asnumpy(y.unique()))\n",
    "    missing_classes = all_classes - present_classes\n",
    "\n",
    "    for label in missing_classes:\n",
    "        class_df = combined_df[combined_df[target_column] == label]\n",
    "        best_idx = class_df['row_mean'].to_pandas().idxmax()\n",
    "        best_sample = class_df.loc[[best_idx]]\n",
    "        filtered_df = cudf.concat([filtered_df, best_sample])\n",
    "\n",
    "    # Step 7: Final separation\n",
    "    X_filtered = filtered_df.drop([target_column, 'row_mean', 'threshold'], axis=1)\n",
    "    y_filtered = filtered_df[target_column]\n",
    "\n",
    "    return X_filtered, y_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply frequency encoding to the training set\n",
    "print('convert_categorical_to_frequency')\n",
    "X_train_filtered_encoded, cat_map, row_map = convert_categorical_to_frequency(X_train_reduced)\n",
    "\n",
    "# Apply per-class row filtering\n",
    "print('filter_low_mean_samples')\n",
    "X_train_filtered, y_train_filtered = filter_low_mean_samples(\n",
    "    X_train_filtered_encoded, \n",
    "    y_train_sampled, \n",
    "    sample_filtering_quantile\n",
    ")\n",
    "\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns)\n",
    "\n",
    "# Revert frequency encoding\n",
    "print('revert_frequency_encoding')\n",
    "# Subset row_map to match the filtered row indices\n",
    "filtered_row_map = {col: row_map[col].loc[X_train_filtered.index] for col in row_map}\n",
    "X_train_filtered = revert_frequency_encoding(X_train_filtered, row_map)\n",
    "fill_categorical_nas([X_train_filtered])\n",
    "\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original training set size: {len(X_train_reduced)} rows\")\n",
    "print(f\"Filtered training set size: {len(X_train_filtered)} rows\")\n",
    "\n",
    "print(f\"\\nClass distribution before filtering:\")\n",
    "print(y_train_sampled.value_counts().sort_index())\n",
    "print(f\"\\nClass distribution after filtering:\")\n",
    "print(y_train_filtered.value_counts().sort_index())\n",
    "\n",
    "# Final validations\n",
    "assert len(X_train_filtered) == len(y_train_filtered), \"Sample count mistmatch between X and y\"\n",
    "assert len(X_train_filtered.columns) == len(X_train_reduced.columns), \"Column count changed\"\n",
    "assert set(X_train_filtered.columns) == set(X_train_reduced.columns), \"Column names changed\"\n",
    "assert y_train_filtered.nunique() == y_train_sampled.nunique(), \"Class count changed\"\n",
    "assert y_train_filtered.nunique() == df_full[target_column].nunique(), \"Some classes are missing\"\n",
    "assert all(y_train_filtered.to_pandas().value_counts() >= 1), \"Some classes have zero samples\"\n",
    "assert X_train_filtered.index.is_unique, \"Duplicate indices in filtered data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d9ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using train_xgb function\n",
    "clf_filtered, train_time_filtered, latency_filtered, f1_weighted_filtered, model_size_filtered = train_xgb(\n",
    "    X_train_filtered, X_val_reduced, X_test_reduced,\n",
    "    y_train_filtered, y_val_sampled, y_test_sampled,\n",
    "    cv=False,  # Cross-validation only used during HPO\n",
    "    metrics_filename=f\"{output_folder}/xgb_filtered_metrics.json\",\n",
    "    cm_filename=f\"{output_folder}/xgb_filtered_cm.csv\",\n",
    "    model_filename=f\"{output_folder}/xgb_filtered_model.json\"\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"Training Time (filtered): {train_time_filtered:.3f} seconds\")\n",
    "print(f\"Latency (filtered): {latency_filtered:.2e} seconds\")\n",
    "print(f\"Weighted F1-Score (filtered): {f1_weighted_filtered:.6f}\")\n",
    "print(f\"Model Size (filtered): {model_size_filtered} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127cd99c-57fd-4616-81f9-50a9329aeb6e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Step 7: HPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52c600-514b-4f36-adb5-b41ffbd48d00",
   "metadata": {},
   "source": [
    "## Step 7.1: Numeric Scaling Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba930803-3ceb-43b3-a612-566378268917",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "scaling_methods = ['none', 'maxabs', 'minmax', 'norm', 'robust', 'standard'] if numeric_columns else ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae8e09e-9bb3-429a-9bcd-401767bc55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import MaxAbsScaler, MinMaxScaler, Normalizer, RobustScaler, StandardScaler\n",
    "\n",
    "def make_numeric_scaler(scaling_method, X):\n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "    non_numeric_columns = X.columns.difference(numeric_columns)\n",
    "\n",
    "    scalers = {\n",
    "        'none': None,\n",
    "        'maxabs': MaxAbsScaler(),\n",
    "        'minmax': MinMaxScaler(),\n",
    "        'norm': Normalizer(),\n",
    "        'robust': RobustScaler(),\n",
    "        'standard': StandardScaler()\n",
    "    }\n",
    "\n",
    "    scaler = scalers.get(scaling_method)\n",
    "\n",
    "    def scale_numeric_features(X_input, fit=False):\n",
    "        X_numeric_pd = X_input[numeric_columns].copy()\n",
    "        X_non_numeric_pd = X_input[non_numeric_columns].copy()\n",
    "        X_output = None\n",
    "\n",
    "        if scaling_method == 'none':\n",
    "            X_output = X_input\n",
    "        else:\n",
    "            if fit:\n",
    "                scaler.fit(X_numeric_pd)\n",
    "            X_numeric_scaled_pd = scaler.transform(X_numeric_pd)\n",
    "            X_numeric_scaled_cu = X_numeric_scaled_pd.astype('float32') # cudf.from_pandas(X_numeric_scaled_pd)\n",
    "            X_numeric_scaled_cu.columns = list(numeric_columns)\n",
    "            X_numeric_scaled_cu.index = X_input.index\n",
    "            # X_numeric_scaled_cu = X_numeric_scaled_cu.reset_index(drop=True)\n",
    "            X_non_numeric_cu = X_non_numeric_pd # cudf.from_pandas(X_non_numeric_pd)\n",
    "            X_non_numeric_cu.columns = list(non_numeric_columns)\n",
    "            X_non_numeric_cu.index = X_input.index\n",
    "            # X_non_numeric_cu = X_non_numeric_cu.reset_index(drop=True)\n",
    "            X_output = cudf.concat([X_numeric_scaled_cu, X_non_numeric_cu], axis=1)\n",
    "            assert X_input.shape == X_output.shape, \"Shape mismatch after concat.\"\n",
    "\n",
    "        return X_output\n",
    "\n",
    "    return scale_numeric_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc40d7-8aa4-4a77-a6a0-505cbfa10187",
   "metadata": {},
   "source": [
    "## Step 7.2: Categorical Encoding Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb9de6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "encoding_methods = ['none', 'onehot', 'ordinal', 'frequency'] if categorical_cols else ['none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2e9173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import LabelEncoder as CumlLabelEncoder\n",
    "from cuml.preprocessing import OneHotEncoder as CumlOneHotEncoder\n",
    "\n",
    "# Define categorical encoding function\n",
    "def make_categorical_encoder(encoding_method, X, max_categories_for_onehot=10):\n",
    "    \"\"\"Creates an encoder for categorical features while keeping numerical columns unchanged.\"\"\"\n",
    "\n",
    "    categorical_columns = X.select_dtypes(include=['category']).columns\n",
    "    numeric_columns = X.columns.difference(categorical_columns)  # Numeric columns remain unchanged\n",
    "\n",
    "    # Split categorical columns for One-Hot vs. None Encoding\n",
    "    onehot_columns = [col for col in categorical_columns if X[col].nunique() <= max_categories_for_onehot]\n",
    "    none_columns = list(set(categorical_columns) - set(onehot_columns))  # High-cardinality features\n",
    "\n",
    "    # Ensure onehot_columns is a valid list (avoiding Index issues)\n",
    "    onehot_columns = list(onehot_columns) if hasattr(onehot_columns, 'tolist') else onehot_columns\n",
    "\n",
    "    # Define encoding strategies\n",
    "    encoders = {\n",
    "        'none': None,   # No transformation (Handled separately)\n",
    "        'onehot': CumlOneHotEncoder(handle_unknown='ignore', sparse=False, sparse_output=False),\n",
    "        'ordinal': CumlLabelEncoder(handle_unknown='ignore'),\n",
    "        'frequency': None  # Frequency encoding requires custom logic\n",
    "    }\n",
    "\n",
    "    encoder = encoders.get(encoding_method)\n",
    "\n",
    "    # Store frequency map for reuse\n",
    "    frequency_col_map, frequency_row_map = {}, {}\n",
    "\n",
    "    def encode_categorical_features(X_input, fit=False):\n",
    "        nonlocal encoder\n",
    "        \"\"\"Encodes categorical features while keeping numeric features unchanged.\"\"\"\n",
    "        _categorical_columns = X_input.select_dtypes(include=['category']).columns\n",
    "        _numeric_columns = X_input.columns.difference(_categorical_columns)\n",
    "        X_numeric = X_input[_numeric_columns].copy()\n",
    "        X_categorical = X_input[_categorical_columns].copy()\n",
    "\n",
    "        if encoding_method == 'none':\n",
    "            # Convert categorical columns to category dtype and assign codes\n",
    "            # X_categorical = X_categorical.apply(lambda col: col.astype('category').cat.codes)\n",
    "            return X_input\n",
    "\n",
    "        elif encoding_method == 'onehot':\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "            # Handle categorical columns together for cuML\n",
    "            if fit:\n",
    "                encoder.fit(X_categorical[onehot_columns])\n",
    "            X_onehot = cudf.DataFrame(\n",
    "                data=encoder.transform(X_categorical[onehot_columns]).astype('int8'), \n",
    "                columns=encoder.get_feature_names(onehot_columns)\n",
    "            )\n",
    "            # Ensure index alignment\n",
    "            X_onehot.index = X_numeric_copy.index  \n",
    "            # Add encoded columns to result\n",
    "            X_encoded = cudf.concat([X_numeric_copy, X_onehot], axis=1)\n",
    "\n",
    "            # Apply 'none' encoding (category codes) for high-cardinality features\n",
    "            if none_columns:\n",
    "                # X_none_encoded = X_categorical[none_columns].apply(lambda col: col.cat.codes)\n",
    "                X_encoded = cudf.concat([X_encoded, X_input[none_columns]], axis=1)\n",
    "\n",
    "            return X_encoded\n",
    "\n",
    "        elif encoding_method == 'ordinal':\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "            X_ordinal_partials = []\n",
    "            # Handle each categorical column separately for cuML\n",
    "            for col in X_categorical.columns:\n",
    "                if fit:\n",
    "                    encoder.fit(X_categorical[col])\n",
    "                X_ordinal = cudf.DataFrame(\n",
    "                    data=encoder.transform(X_categorical[col]).astype('category'),\n",
    "                    columns=[col]\n",
    "                )\n",
    "                # Ensure index alignment\n",
    "                X_ordinal.index = X_numeric_copy.index  \n",
    "                # Add encoded columns to result\n",
    "                X_ordinal_partials.append(X_ordinal)\n",
    "            X_encoded = cudf.concat([X_numeric_copy, cudf.concat(X_ordinal_partials, axis=1)], axis=1)\n",
    "\n",
    "            return X_encoded\n",
    "\n",
    "        elif encoding_method == 'frequency':\n",
    "            nonlocal frequency_col_map, frequency_row_map\n",
    "            X_numeric_copy = X_numeric.copy()  # Start with numeric columns\n",
    "\n",
    "            if fit:\n",
    "                # Fit and encode\n",
    "                X_categorical_encoded, frequency_col_map, frequency_row_map = \\\n",
    "                    convert_categorical_to_frequency(X_categorical)\n",
    "            else:\n",
    "                # Transform using saved frequency map\n",
    "                X_categorical_encoded = X_categorical.copy()\n",
    "                for col in X_categorical.columns:\n",
    "                    mapped = X_categorical[col].to_pandas().map(frequency_col_map.get(col, {}))\n",
    "                    X_categorical_encoded[col] = cudf.Series(mapped, index=X_categorical.index).astype('float32')\n",
    "\n",
    "            return cudf.concat([X_numeric_copy, X_categorical_encoded], axis=1)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoding method: {encoding_method}\")\n",
    "\n",
    "        # Ensure all categorical columns are replaced by their codes\n",
    "        # X_categorical = X_categorical.apply(lambda col: col.cat.codes)\n",
    "\n",
    "    return encode_categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15172bb4-7dab-4b34-a6b3-7732dbfbe7ab",
   "metadata": {},
   "source": [
    "## Step 7.3: Data Balancing Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68333e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "SAFE_MODE = False\n",
    "\n",
    "def assert_or_raise(condition: bool, message: str):\n",
    "    if SAFE_MODE and not condition:\n",
    "        raise ValueError(message)\n",
    "\n",
    "class CumlRandomResampler:\n",
    "    def __init__(self, strategy: dict, sampling_type: str = 'over'):\n",
    "        assert sampling_type in ['over', 'under'], \"sampling_type must be 'over' or 'under'\"\n",
    "        self.strategy = strategy\n",
    "        self.sampling_type = sampling_type\n",
    "\n",
    "    def fit_resample(self, X: cudf.DataFrame, y: cudf.Series):\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "\n",
    "        df = X.copy()\n",
    "        df['__label__'] = y\n",
    "\n",
    "        resampled_dfs = []\n",
    "\n",
    "        for cls in df['__label__'].to_pandas().unique().tolist():\n",
    "            cls_df = df[df['__label__'] == cls]\n",
    "            count = len(cls_df)\n",
    "\n",
    "            if self.sampling_type == 'over' and cls in self.strategy:\n",
    "                target_total = self.strategy[cls]\n",
    "                current_count = len(cls_df)\n",
    "                to_add = target_total - current_count\n",
    "\n",
    "                if to_add > 0:\n",
    "                    extra = cls_df.sample(n=to_add, replace=True, random_state=42)\n",
    "                    cls_df = cudf.concat([cls_df, extra], ignore_index=True)\n",
    "                    assert_or_raise(cls_df.shape[0] == target_total,\n",
    "                                    f\"Over-sampling failed for class {cls}: expected {target_total}, got {cls_df.shape[0]}\")\n",
    "                    assert_or_raise(cls_df.shape[1] == df.shape[1],\n",
    "                                    f\"Column mismatch after over-sampling class {cls}\")\n",
    "\n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "            elif self.sampling_type == 'under' and cls in self.strategy:\n",
    "                target_keep = self.strategy[cls]\n",
    "                to_remove = count - target_keep\n",
    "\n",
    "                if to_remove >= count:\n",
    "                    # Skip appending — fully removed\n",
    "                    print(f\"[Resampler] Class {cls} will be removed entirely (requested to remove {to_remove}, only {count} available)\")\n",
    "                    continue\n",
    "\n",
    "                if to_remove > 0:\n",
    "                    cls_df = cls_df.sample(n=target_keep, replace=False, random_state=42)\n",
    "                    expected_rows = target_keep\n",
    "                    expected_cols = df[df['__label__'] == cls].shape[1]\n",
    "                    assert_or_raise(cls_df.shape[0] == expected_rows,\n",
    "                                    f\"Under-sampling failed for class {cls}: expected {expected_rows}, got {cls_df.shape[0]}\")\n",
    "                    assert_or_raise(cls_df.shape[1] == expected_cols,\n",
    "                                    f\"Column mismatch after under-sampling class {cls}\")\n",
    "                \n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "            else:\n",
    "                # Not in strategy → keep class as-is\n",
    "                resampled_dfs.append(cls_df)\n",
    "\n",
    "        result_df = cudf.concat(resampled_dfs, ignore_index=True)\n",
    "        X_resampled = result_df.drop(columns='__label__')\n",
    "        y_resampled = result_df['__label__']\n",
    "\n",
    "        # Final checks\n",
    "\n",
    "        expected_total = sum(self.strategy.values())\n",
    "        expected_classes = set(y.to_pandas().unique())\n",
    "        actual_classes = set(y_resampled.to_pandas().unique())\n",
    "        assert_or_raise(X_resampled.shape[0] == expected_total,\n",
    "                        f\"{self.sampling_type.capitalize()}-sampling total row count mismatch: expected {expected_total}, got {X_resampled.shape[0]}\")\n",
    "        assert_or_raise(y_resampled.shape[0] == expected_total,\n",
    "                        f\"{self.sampling_type.capitalize()}-sampling total label count mismatch: expected {expected_total}, got {y_resampled.shape[0]}\")\n",
    "        assert_or_raise(X_resampled.shape[1] == X.shape[1],\n",
    "                        f\"Feature count mismatch after resampling: expected {X.shape[1]}, got {X_resampled.shape[1]}\")\n",
    "        assert_or_raise(actual_classes == expected_classes,\n",
    "                        f\"Class mismatch after resampling: expected {expected_classes}, got {actual_classes}\")\n",
    "\n",
    "        return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721b475",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors as CumlNearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a822a-955d-4685-9c3e-d89b57407741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_resample(_X_train, _y_train, over_method, over_thresh, under_method, under_thresh):\n",
    "\n",
    "    _X_names = _X_train.columns.tolist()\n",
    "    _y_name = _y_train.name\n",
    "\n",
    "    _X_train_copy = _X_train.copy()\n",
    "    _y_train_copy = _y_train.copy()\n",
    "\n",
    "    if over_thresh:\n",
    "        value_counts = _y_train_copy.value_counts().to_dict()\n",
    "        n_neighbors = min(5, min(value_counts.values()))\n",
    "        n_generate = build_oversampling_strategy(value_counts, over_thresh)\n",
    "        over_strategy = patch_oversampling_strategy(value_counts, n_generate)\n",
    "        cat_features = [\n",
    "            _X_train_copy.columns.get_loc(col)\n",
    "            for col in _X_train_copy.select_dtypes(include=['category']).columns\n",
    "        ]\n",
    "        over_sampler = make_over_sampler(over_method, over_strategy, n_neighbors, cat_features)\n",
    "        _X_train_copy, _y_train_copy = over_sampler.fit_resample(_X_train_copy, _y_train_copy)\n",
    "\n",
    "    if under_thresh:\n",
    "        value_counts = _y_train_copy.value_counts().to_dict()\n",
    "        n_remove = build_undersampling_strategy(value_counts, under_thresh)\n",
    "        under_strategy = patch_undersampling_strategy(value_counts, n_remove)\n",
    "        under_sampler = make_under_sampler(under_method, under_strategy)\n",
    "        _X_train_copy, _y_train_copy = under_sampler.fit_resample(_X_train_copy, _y_train_copy)\n",
    "\n",
    "    return cudf.DataFrame(_X_train_copy, columns=_X_names), cudf.Series(_y_train_copy, name=_y_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282e984-eb6a-44d8-bb6b-131a2630e07e",
   "metadata": {},
   "source": [
    "### Step 7.3a: Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc68786",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "over_method_choices = ['random']#, 'smotenc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216115dc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "over_threshold_choices = [float(f) for f in np.linspace(0, 4, num=17).round(2)] + ['auto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace42fb-1289-489a-9223-d755eb5406d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, SMOTENC, RandomOverSampler\n",
    "\n",
    "def build_oversampling_strategy(value_counts, threshold):\n",
    "    n_occurrences = sum(value_counts.values())\n",
    "    perfectly_balanced_occurrences = int(n_occurrences / len(value_counts.keys()))\n",
    "\n",
    "    if threshold == \"auto\":\n",
    "        n_generate = {\n",
    "            class_: perfectly_balanced_occurrences - occ\n",
    "                    if occ < perfectly_balanced_occurrences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    else:\n",
    "        n_generate = {\n",
    "            class_: int(min(occ * threshold, perfectly_balanced_occurrences - occ))\n",
    "                    if occ < perfectly_balanced_occurrences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    return n_generate\n",
    "\n",
    "def patch_oversampling_strategy(value_counts, n_generate):\n",
    "    return {k: (value_counts[k] + n_generate[k]) for k in value_counts.keys()}\n",
    "\n",
    "def make_over_sampler(over_method, over_strategy, n_neighbors, cat_features=None):\n",
    "    if over_method == \"random\":\n",
    "        return CumlRandomResampler(strategy=over_strategy, sampling_type=\"over\")\n",
    "    elif over_method == \"smote\":\n",
    "        return SMOTE(k_neighbors=CumlNearestNeighbors(n_neighbors=n_neighbors), sampling_strategy=over_strategy)\n",
    "    elif over_method == \"smotenc\":\n",
    "        return SMOTENC(categorical_features=cat_features, k_neighbors=CumlNearestNeighbors(n_neighbors=n_neighbors), sampling_strategy=over_strategy)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown oversampling method: {over_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c917b490-3e36-4256-bbf0-961a507c5307",
   "metadata": {},
   "source": [
    "### Step 7.3b: Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e61da",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "under_method_choices = ['random']#, 'tomek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c0b253",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "under_threshold_choices = [float(f) for f in np.linspace(0, 0.95, num=20).round(2)] + ['auto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbc59bd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks as TomekLinksImblearn\n",
    "from sklearn.utils import _safe_indexing  # Needed for compatibility\n",
    "\n",
    "class TomekLinksCUDA(TomekLinksImblearn):\n",
    "    def fit_resample(self, X, y):\n",
    "        nn = CumlNearestNeighbors(n_neighbors=2)\n",
    "        nn.fit(X)\n",
    "        nns = nn.kneighbors(X, return_distance=False)[:, 1]\n",
    "\n",
    "        links = self.is_tomek(y, nns, self.sampling_strategy_)\n",
    "        self.sample_indices_ = np.flatnonzero(np.logical_not(links))\n",
    "\n",
    "        return (\n",
    "            _safe_indexing(X, self.sample_indices_),\n",
    "            _safe_indexing(y, self.sample_indices_),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0899ed9-91b9-48e7-99ff-1635ffd3d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def build_undersampling_strategy(value_counts, threshold):\n",
    "    n_occurences = sum([n for n in value_counts.values()])\n",
    "    perfectly_balanced_occurences = int(n_occurences / len(value_counts.keys()))\n",
    "    if threshold == \"auto\":\n",
    "        n_remove = {\n",
    "            class_: occ - perfectly_balanced_occurences\n",
    "                    if occ > perfectly_balanced_occurences else 0\n",
    "                    for class_, occ in value_counts.items()\n",
    "        }\n",
    "    else:\n",
    "        n_remove = {\n",
    "            class_: int(min(occ * threshold, occ - perfectly_balanced_occurences))\n",
    "            if occ > perfectly_balanced_occurences else 0\n",
    "            for class_, occ in value_counts.items()\n",
    "        }\n",
    "    return n_remove\n",
    "\n",
    "def patch_undersampling_strategy(value_counts, n_remove):\n",
    "    return {k : (value_counts[k] - n_remove[k]) for k in value_counts.keys()}\n",
    "\n",
    "def make_under_sampler(under_method, under_strategy):\n",
    "    if under_method == \"random\":\n",
    "        return CumlRandomResampler(strategy=under_strategy, sampling_type=\"under\")\n",
    "    elif under_method == \"tomek\":\n",
    "        return TomekLinksCUDA(n_jobs=n_jobs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown undersampling method: {under_method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36776a83-f2ec-47a5-b036-b79ce314593c",
   "metadata": {},
   "source": [
    "## Step 7.4: Study Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06241f6-6166-4db9-ba53-2d31dc99530d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "y_train_filtered = cudf.Series(y_train_filtered)\n",
    "\n",
    "if y_train_filtered.value_counts().min() < min_samples_per_class:\n",
    "    df_temp = cudf.concat([\n",
    "        X_train_filtered.reset_index(drop=True),\n",
    "        y_train_filtered.reset_index(drop=True)\n",
    "        ], axis='columns')\n",
    "    df_temp = ensure_min_samples_per_class(df_temp, target_column, min_samples_per_class, random_state)\n",
    "    X_train_filtered, y_train_filtered = df_temp.drop(columns=[target_column]), df_temp[target_column]\n",
    "    assert y_train_filtered.value_counts().min() >= min_samples_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd1c98-6305-4084-bbb8-cebc37266a0e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "assert isinstance(X_train_filtered, cudf.DataFrame)\n",
    "assert isinstance(y_train_filtered, cudf.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ed33b-ce61-444d-a77b-e2445eb38625",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "# Define objective function for Optuna HPO\n",
    "def objective(trial):\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Apply Data Balancing\n",
    "\n",
    "        X_dtypes_before = X_train_filtered.dtypes.copy(deep=True).sort_index()\n",
    "        y_dtype_before = y_train_filtered.dtype\n",
    "        \n",
    "        balancing_mode = trial.suggest_categorical(\"balancing_mode\", [\"none\", \"resampling\", \"weighting\"])\n",
    "        sample_weights = None  # Default\n",
    "\n",
    "        if balancing_mode == \"none\":\n",
    "            X_train_resampled, y_train_resampled = X_train_filtered, y_train_filtered\n",
    "        \n",
    "        elif balancing_mode == \"resampling\":\n",
    "            over_method = trial.suggest_categorical(\"over_method\", over_method_choices)\n",
    "            over_threshold = trial.suggest_categorical(\"over_threshold\", over_threshold_choices)\n",
    "            under_method = trial.suggest_categorical(\"under_method\", under_method_choices)\n",
    "            under_threshold = trial.suggest_categorical(\"under_threshold\", under_threshold_choices)\n",
    "        \n",
    "            # Resample the training set\n",
    "            X_train_resampled, y_train_resampled = fit_resample(\n",
    "                X_train_filtered, y_train_filtered,\n",
    "                over_method, over_threshold,\n",
    "                under_method, under_threshold\n",
    "            )\n",
    "        \n",
    "        elif balancing_mode == \"weighting\":\n",
    "            X_train_resampled, y_train_resampled = X_train_filtered, y_train_filtered\n",
    "        \n",
    "            # Compute inverse-frequency sample weights\n",
    "            class_counts = y_train_resampled.value_counts()\n",
    "            weight_map = 1.0 / class_counts\n",
    "            sample_weights = y_train_resampled.map(weight_map)\n",
    "        \n",
    "        X_dtypes_after = X_train_resampled.dtypes.copy(deep=True).sort_index()\n",
    "        y_dtype_after = y_train_resampled.dtype\n",
    "\n",
    "        assert isinstance(X_train_resampled, cudf.DataFrame)\n",
    "        assert isinstance(y_train_resampled, cudf.Series)\n",
    "        assert X_dtypes_before.equals(X_dtypes_after), \"X dtypes mismatch after resampling\"\n",
    "        assert y_dtype_before == y_dtype_after, \"y dtype mismatch after resampling\"\n",
    "        assert X_train_resampled.shape[0] == y_train_resampled.shape[0]\n",
    "        assert X_train_resampled.shape[1] == X_train_filtered.shape[1]\n",
    "        assert X_train_resampled.isna().sum().sum() == 0\n",
    "        assert y_train_resampled.isna().sum().sum() == 0\n",
    "        assert set(y_train_resampled.to_pandas().unique()) == set(df_full[target_column].to_pandas().unique())\n",
    "\n",
    "        # Classifier hyperparameters (to be tuned)\n",
    "        hpo_booster_params = {\n",
    "            \"eta\": trial.suggest_float(\"eta\", 0.01, 0.3, log=True),                # Learning rate, controls step size at each iteration\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 8),                     # Maximum depth of trees, prevents overfitting\n",
    "            \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),      # Minimum sum of instance weight needed in a child node\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),               # Fraction of training data used per boosting round\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0), # Fraction of features used per tree\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 10.0, log=True),         # L2 regularization term (Ridge regression)\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True),           # L1 regularization term (Lasso regression)\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 5.0)                          # Minimum loss reduction to make a split\n",
    "        }\n",
    "\n",
    "        # Perform cross-validation using train_xgb function with custom booster params\n",
    "        trial_str = str(trial.number).zfill(len(str(hpo_n_trials)))\n",
    "        model_hpo, train_time_hpo, latency_hpo, f1_weighted_hpo, model_size_hpo = \\\n",
    "            train_xgb(\n",
    "                X_train_resampled, None, X_val_reduced,   # No validation set since CV == True\n",
    "                y_train_resampled, None, y_val_sampled,   # No validation set since CV == True\n",
    "                sample_weights=sample_weights,            # Depends on balancing strategy\n",
    "                cv=True,                                  # Enable cross-validation\n",
    "                custom_booster_params=hpo_booster_params, # Pass the tuned parameters\n",
    "                verbose=0,                                # Suppress CV output during HPO\n",
    "                metrics_filename=f\"{output_folder}/xgb_hpo_trial_{trial_str}_metrics.csv\"\n",
    "            )\n",
    "\n",
    "        return latency_hpo, f1_weighted_hpo\n",
    "\n",
    "    except (ValueError, Exception) as e:\n",
    "        print(e)\n",
    "        trial_params = \", \".join(f\"{k}: {v}\" for k, v in trial.params.items())\n",
    "        raise optuna.TrialPruned(f\"{trial_params} => {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c78e02-4c79-447a-a234-3f5cd97f19d9",
   "metadata": {},
   "source": [
    "## Step 7.5: Study Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9369c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Run Optuna optimization\n",
    "start_time = time.time()\n",
    "sampler = optuna.samplers.TPESampler(multivariate=True, group=True, seed=random_state)\n",
    "study = optuna.create_study(directions=[\"minimize\", \"maximize\"], sampler=sampler)\n",
    "study.optimize(objective, n_trials=hpo_n_trials, timeout=hpo_timeout)\n",
    "hpo_time = time.time() - start_time\n",
    "print(f\"Execution Time (HPO): {hpo_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa7c3f-48c9-4700-a415-18ced0a68a0c",
   "metadata": {},
   "source": [
    "## Step 7.6: Results Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ae64d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_hpo = study.trials_dataframe()\n",
    "\n",
    "col_order = ['number', 'datetime_start', 'datetime_complete', 'duration', 'latency', 'f1_score',\n",
    "             \"balancing_mode\", \"over_method\", \"over_threshold\", \"under_method\", \"under_threshold\",\n",
    "             \"eta\", \"max_depth\", \"min_child_weight\", \"subsample\", \"colsample_bytree\", \"lambda\", \"alpha\"]\n",
    "\n",
    "available_cols = [c for c in col_order if c in df_hpo.columns or f'params_{c}' in df_hpo.columns]\n",
    "df_hpo_clean = df_hpo.rename(columns={'values_0': 'latency', 'values_1': 'f1_score'})\n",
    "df_hpo_clean = df_hpo_clean.rename(columns={c: c.replace(\"params_\", \"\") for c in df_hpo_clean.columns})\n",
    "\n",
    "df_hpo_styled = df_hpo_clean \\\n",
    "    .rename(columns={'values_0': 'latency', 'values_1': 'f1_score'}) \\\n",
    "    .rename(columns={c : c.replace('params_', '') for c in df_hpo.columns})[col_order].style \\\n",
    "    .background_gradient(cmap=\"RdYlGn_r\", subset=[\"latency\"]) \\\n",
    "    .background_gradient(cmap=\"RdYlGn\", subset=[\"f1_score\"]) \\\n",
    "    .format({\"latency\": \"{:.2e}\", \"f1_score\": \"{:.6f}\"}) \\\n",
    "    .set_table_styles([{\"selector\": \"th, td\", \"props\": \"border: 1px solid black; text-align: center;\"}])\n",
    "\n",
    "df_hpo_styled.to_excel(f\"{output_folder}/xgb_hpo_trials.xlsx\")\n",
    "with open(f\"{output_folder}/xgb_hpo_trials.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(df_hpo_styled.to_html())\n",
    "\n",
    "df_hpo_styled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f800ad-b623-4083-89a7-16f79c062dfd",
   "metadata": {},
   "source": [
    "## Step 7.6a: Optimization History (Best Latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01868ae9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study, target=lambda t: t.values[0], target_name=\"Latency\")\n",
    "fig = go.Figure(fig)\n",
    "fig.update_layout(width=None, height=None, autosize=True)\n",
    "fig.write_html(f\"{output_folder}/xgb_hpo_optimization_history_best_latency.html\", full_html=True)\n",
    "fig.write_image(f\"{output_folder}/xgb_hpo_optimization_history_best_latency.pdf\", width=1920, height=1080)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f86a53-7571-40f0-8673-fe42dc70e753",
   "metadata": {},
   "source": [
    "## Step 7.6b: Optimization History (Best Weighted F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c18166b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study, target=lambda t: t.values[1], target_name=\"Weighted F1 Score\")\n",
    "fig = go.Figure(fig)\n",
    "fig.update_layout(width=None, height=None, autosize=True)\n",
    "fig.write_image(f\"{output_folder}/xgb_hpo_optimization_history_best_weighted_f1_score.pdf\", width=1920, height=1080)\n",
    "fig.write_html(f\"{output_folder}/xgb_hpo_optimization_history_best_weighted_f1_score.html\", full_html=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a49be-c389-4554-8162-dad427281e28",
   "metadata": {},
   "source": [
    "## Step 7.6c: Pareto Front (Best Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b6ee7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_pareto_front(points):\n",
    "    \"\"\"Extracts the Pareto front from a set of (latency, F1-score) points.\"\"\"\n",
    "    pareto_front = []\n",
    "    points = sorted(points, key=lambda x: x[0])  # Sort by latency (x-axis)\n",
    "\n",
    "    max_f1 = -np.inf\n",
    "    for x, y in points:\n",
    "        if y > max_f1:  # A better (higher) F1-score found\n",
    "            pareto_front.append((x, y))\n",
    "            max_f1 = y\n",
    "\n",
    "    return np.array(pareto_front)\n",
    "\n",
    "# Generate Pareto front plot\n",
    "fig = optuna.visualization.plot_pareto_front(study, target_names=[\"Latency\", \"Weighted F1 Score\"])\n",
    "fig = go.Figure(fig)\n",
    "\n",
    "# Extract study results (Latency = values[0], F1 Score = values[1])\n",
    "all_points = np.array([(t.values[0], t.values[1]) for t in study.trials if t.values])\n",
    "\n",
    "# Get only the Pareto-optimal points\n",
    "pareto_points = get_pareto_front(all_points)\n",
    "\n",
    "# Add Pareto front as a semi-transparent line\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=pareto_points[:, 0], \n",
    "        y=pareto_points[:, 1], \n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", width=3, dash=\"solid\"),\n",
    "        opacity=0.2,  # Set opacity (0 = fully transparent, 1 = fully opaque)\n",
    "        name=\"Pareto Front\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        x=1.05,  # Horizontal position (0 = left, 1 = right)\n",
    "        y=1.15,  # Vertical position (0 = bottom, 1 = top)\n",
    "        bgcolor=\"rgba(255, 255, 255, 0.8)\",  # Semi-transparent white background\n",
    "        bordercolor=\"black\",  # Optional: Add a border\n",
    "        borderwidth=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Invert x-axis so lower latency is on the right\n",
    "fig.update_layout(xaxis=dict(autorange=\"reversed\"), width=None, height=None, autosize=True)\n",
    "\n",
    "# Save and show plot\n",
    "fig.write_html(f\"{output_folder}/xgb_pareto_front.html\", full_html=True)\n",
    "fig.write_image(f\"{output_folder}/xgb_pareto_front.pdf\", width=1920, height=1080)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af32a2f7-4a60-441c-b0c8-52609779034f",
   "metadata": {},
   "source": [
    "## Step 7.6d: Parallel Coordinates (Best Latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c5fa4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_parallel_coordinate(study, target=lambda t: t.values[0], target_name=\"Latency\")\n",
    "fig = go.Figure(fig)\n",
    "fig.update_layout(width=None, height=None, autosize=True)\n",
    "fig.write_html(f\"{output_folder}/xgb_parallel_coordinate_best_latency.html\", full_html=True)\n",
    "fig.write_image(f\"{output_folder}/xgb_parallel_coordinate_best_latency.pdf\", width=1920, height=1080)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1df95-9139-49c3-8c99-24ae8869e0b4",
   "metadata": {},
   "source": [
    "## Step 7.6e: Parallel Coordinates (Best Weighted F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906982f6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_parallel_coordinate(study, target=lambda t: t.values[1], target_name=\"Weighted F1 Score\")\n",
    "fig = go.Figure(fig)\n",
    "fig.update_layout(width=None, height=None, autosize=True)\n",
    "fig.write_html(f\"{output_folder}/xgb_parallel_coordinate_best_weighted_f1_score.html\", full_html=True)\n",
    "fig.write_image(f\"{output_folder}/xgb_parallel_coordinate_best_weighted_f1_score.pdf\", width=1920, height=1080)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba247d56-33f6-4568-9b91-e4d410421e85",
   "metadata": {},
   "source": [
    "## Step 7.7: Param Importances (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60984db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_param_importances:\n",
    "\n",
    "    from plotly.subplots import make_subplots\n",
    "\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=[\"Latency\", \"Weighted F1 Score\"])\n",
    "    fig = go.Figure(fig)\n",
    "\n",
    "    # Generate parameter importance plot for Latency\n",
    "    fig_latency = optuna.visualization.plot_param_importances(\n",
    "        study, target=lambda t: t.values[0], target_name=\"Latency\"\n",
    "    )\n",
    "    fig_latency.data[0].name = \"Latency\"  # Rename trace\n",
    "    fig.add_trace(fig_latency.data[0], row=1, col=1)\n",
    "\n",
    "    # Generate parameter importance plot for Weighted F1 Score\n",
    "    fig_f1_score = optuna.visualization.plot_param_importances(\n",
    "        study, target=lambda t: t.values[1], target_name=\"Weighted F1 Score\"\n",
    "    )\n",
    "    fig_f1_score.data[0].name = \"Weighted F1 Score\"  # Rename trace\n",
    "    fig.add_trace(fig_f1_score.data[0], row=1, col=2)\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(title_text=\"Hyperparameter Importance for F1 Score & Latency\", width=None, height=None, autosize=True)\n",
    "\n",
    "    # Save and show plot\n",
    "    fig.write_html(f\"{output_folder}/xgb_param_importances.html\", full_html=True)\n",
    "    fig.write_image(f\"{output_folder}/xgb_param_importances.pdf\", width=1920, height=1080)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d22ac0-bd55-4c3f-b5a2-63126fc74803",
   "metadata": {},
   "source": [
    "# Step 8: Pipeline Reconstruction and Refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a79eac-d364-47c5-9ad1-9193b62b3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def reconstruct_and_evaluate_best_model(best_params, optimized_metric):\n",
    "    # Deep copy to avoid modifying original dict\n",
    "    best_params_copy = deepcopy(best_params)\n",
    "\n",
    "    # Extract balancing strategy\n",
    "    balancing_mode = best_params_copy.pop(\"balancing_mode\")\n",
    "    sample_weights = None  # Default is no weighting\n",
    "\n",
    "    # Handle each balancing mode\n",
    "\n",
    "    if balancing_mode == \"none\":\n",
    "        # No resampling or weighting\n",
    "        X_train_final, y_train_final = X_train_filtered, y_train_filtered\n",
    "    \n",
    "    elif balancing_mode == \"resampling\":\n",
    "        # Extract resampling params\n",
    "        best_over_method = best_params_copy.pop(\"over_method\")\n",
    "        best_over_threshold = best_params_copy.pop(\"over_threshold\")\n",
    "        best_under_method = best_params_copy.pop(\"under_method\")\n",
    "        best_under_threshold = best_params_copy.pop(\"under_threshold\")\n",
    "\n",
    "        # Perform resampling\n",
    "        X_train_final, y_train_final = fit_resample(\n",
    "            X_train_filtered, y_train_filtered,\n",
    "            best_over_method, best_over_threshold,\n",
    "            best_under_method, best_under_threshold\n",
    "        )\n",
    "\n",
    "    elif balancing_mode == \"weighting\":\n",
    "        # No resampling; just apply inverse frequency weighting\n",
    "        X_train_final, y_train_final = X_train_filtered, y_train_filtered\n",
    "\n",
    "        class_counts = y_train_final.value_counts()\n",
    "        weight_map = 1.0 / class_counts\n",
    "        sample_weights = y_train_final.map(weight_map)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid balancing_mode: {balancing_mode}\")\n",
    "\n",
    "    # Train final model on full training set\n",
    "    optimized_metric_fmt = f\"best_{optimized_metric.replace(' ', '_').lower()}\"\n",
    "    model_best, train_time_best, latency_best, f1_weighted_best, model_size_best = train_xgb(\n",
    "        X_train_final, X_val_reduced, X_test_reduced,\n",
    "        y_train_final, y_val_sampled, y_test_sampled,\n",
    "        sample_weights=sample_weights,\n",
    "        cv=False,  # Final training, no CV\n",
    "        custom_booster_params=best_params_copy,\n",
    "        metrics_filename=f\"{output_folder}/xgb_{optimized_metric_fmt}_metrics.json\",\n",
    "        cm_filename=f\"{output_folder}/xgb_{optimized_metric_fmt}_cm.csv\",\n",
    "        model_filename=f\"{output_folder}/xgb_{optimized_metric_fmt}_model.json\"\n",
    "    )\n",
    "\n",
    "    # Report\n",
    "    print(f\"Training Time (best {optimized_metric}): {train_time_best:.3f} seconds\")\n",
    "    print(f\"Latency (best {optimized_metric}): {latency_best:.2e} seconds\")\n",
    "    print(f\"Weighted F1-Score (best {optimized_metric}): {f1_weighted_best:.6f}\")\n",
    "    print(f\"Model Size (best {optimized_metric}): {model_size_best} MB\")\n",
    "\n",
    "    return X_train_final, model_size_best, train_time_best, latency_best, f1_weighted_best, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6cd17-b5b5-492d-a868-570b33131b7c",
   "metadata": {},
   "source": [
    "## Step 8.1: Updated Evaluation (with Sampling, Feature Selection, Row Filtering, and HPO) => Lowest Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680bbb9-fcdb-4510-82ab-410e7c8d52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best trial based on chosen strategy (here, minimizing Latency)\n",
    "optimized_metric = 'Latency'\n",
    "best_params = min(study.best_trials, key=lambda t: t.values[0]).params\n",
    "print(f\"Best Hyperparameters for {optimized_metric}:\", best_params)\n",
    "\n",
    "# Reconstruct and evaluate the best model\n",
    "X_train_resampled_best_latency, model_size_best_latency, train_time_best_latency, latency_best_latency, f1_weighted_best_latency, best_params_best_latency = \\\n",
    "    reconstruct_and_evaluate_best_model(best_params, optimized_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bfe86-fb7d-4143-b688-dd0daee0d49d",
   "metadata": {},
   "source": [
    "## Step 8.2: Updated Evaluation (with Sampling, Feature Selection, Row Filtering, and HPO) => Highest Weighted F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90f852-a0e9-4505-8292-7f6ed7be77fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best trial based on chosen strategy (here, maximizing weighted F1 Score)\n",
    "optimized_metric = 'Weighted F1 Score'\n",
    "best_params = max(study.best_trials, key=lambda t: t.values[1]).params\n",
    "print(f\"Best Hyperparameters for {optimized_metric}:\", best_params)\n",
    "\n",
    "# Reconstruct and evaluate the best model\n",
    "X_train_resampled_best_f1_score, model_size_best_f1_score, train_time_best_f1_score, latency_best_f1_score, f1_weighted_best_f1_score, best_params_best_f1_score = \\\n",
    "    reconstruct_and_evaluate_best_model(best_params, optimized_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f72aec6-fcb6-4171-b473-7f0e470b94fb",
   "metadata": {},
   "source": [
    "## Step 8.2: Updated Evaluation (with Sampling, Feature Selection, Row Filtering, and HPO) => Best Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a58a4c-183c-4612-a99e-bdf360d8634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best trial based on chosen strategy (here, maximizing weighted F1 Score)\n",
    "optimized_metric = 'Balance'\n",
    "\n",
    "# Extract Pareto-optimal trials\n",
    "pareto_trials = study.best_trials  \n",
    "\n",
    "# Normalize F1-score and Latency across Pareto-optimal trials\n",
    "latency_values = np.array([t.values[0] for t in pareto_trials])  # Latency (lower is better)\n",
    "f1_values = np.array([t.values[1] for t in pareto_trials])  # F1-score (higher is better)\n",
    "\n",
    "# Min-max normalization for Latency (avoid division by zero, inverted since lower is better)\n",
    "latency_min, latency_max = latency_values.min(), latency_values.max()\n",
    "if latency_max - latency_min == 0:\n",
    "    latency_norm = np.ones_like(latency_values)  # Assign 1 if all values are the same\n",
    "else:\n",
    "    latency_norm = (latency_max - latency_values) / (latency_max - latency_min)  # Inverted\n",
    "\n",
    "# Min-max normalization for F1-score (avoid division by zero)\n",
    "f1_min, f1_max = f1_values.min(), f1_values.max()\n",
    "if f1_max - f1_min == 0:\n",
    "    f1_norm = np.ones_like(f1_values)  # Assign 1 if all values are the same\n",
    "else:\n",
    "    f1_norm = (f1_values - f1_min) / (f1_max - f1_min)\n",
    "\n",
    "# Compute a balanced trade-off score (equal weight for F1 and latency)\n",
    "trade_off_scores = 0.5 * f1_norm + 0.5 * latency_norm\n",
    "\n",
    "# Select the best trial based on the highest trade-off score\n",
    "best_tradeoff_trial = pareto_trials[np.argmax(trade_off_scores)]\n",
    "\n",
    "# Extract best hyperparameters\n",
    "best_params = best_tradeoff_trial.params\n",
    "print(f\"Best Hyperparameters for {optimized_metric}:\", best_params)\n",
    "\n",
    "# Reconstruct and evaluate the best model\n",
    "X_train_resampled_best_balance, model_size_best_balance, train_time_best_balance, latency_best_balance, f1_weighted_best_balance, best_params_best_balance = \\\n",
    "    reconstruct_and_evaluate_best_model(best_params, optimized_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e43bbdc-6f89-4126-8a0f-9316aeb739ec",
   "metadata": {},
   "source": [
    "# Step 9: Execution Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21a6745",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "total_trials = len(study.trials)\n",
    "completed_trials = sum(1 for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE)\n",
    "completion_ratio_str = f\"{completed_trials}/{total_trials} ({hpo_n_trials})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe24bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def round_floats_in_dict(d, decimals=3):\n",
    "    return {\n",
    "        key: (\n",
    "            round_floats_in_dict(value, decimals) if isinstance(value, dict)  # Recursively process dicts\n",
    "            else round(value, decimals) if isinstance(value, float)  # Round floats\n",
    "            else value  # Keep everything else unchanged\n",
    "        )\n",
    "        for key, value in d.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1fa58-6531-48b3-9bad-d16a98a858dd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'X_train_shape': {\n",
    "        'full': X_train_full.shape,\n",
    "        'sampled': X_train_sampled.shape,\n",
    "        'reduced': X_train_reduced.shape,\n",
    "        'filtered': X_train_filtered.shape,\n",
    "        'hpo_best_latency': X_train_resampled_best_latency.shape,\n",
    "        'hpo_best_f1_score': X_train_resampled_best_f1_score.shape,\n",
    "        'hpo_best_balance': X_train_resampled_best_balance.shape\n",
    "    },\n",
    "    'X_val_shape': {\n",
    "        'full': X_val_full.shape,\n",
    "        'sampled': X_val_sampled.shape,\n",
    "        'reduced': X_val_reduced.shape,\n",
    "        'filtered': X_val_reduced.shape,\n",
    "        'hpo_best_latency': X_val_reduced.shape,\n",
    "        'hpo_best_f1_score': X_val_reduced.shape,\n",
    "        'hpo_best_balance': X_val_reduced.shape\n",
    "    },\n",
    "    'X_test_shape': {\n",
    "        'full': X_test_full.shape,\n",
    "        'sampled': X_test_sampled.shape,\n",
    "        'reduced': X_test_reduced.shape,\n",
    "        'filtered': X_test_reduced.shape,\n",
    "        'hpo_best_latency': X_test_reduced.shape,\n",
    "        'hpo_best_f1_score': X_test_reduced.shape,\n",
    "        'hpo_best_balance': X_test_reduced.shape\n",
    "    },\n",
    "    'model_size_abs': {\n",
    "        'full': round(model_size_full, 2),\n",
    "        'sampled': round(model_size_sampled, 2),\n",
    "        'reduced': round(model_size_reduced, 2),\n",
    "        'filtered': round(model_size_filtered, 2),\n",
    "        'hpo_best_latency': round(model_size_best_latency, 2),\n",
    "        'hpo_best_f1_score': round(model_size_best_f1_score, 2),\n",
    "        'hpo_best_balance': round(model_size_best_balance, 2)\n",
    "    },\n",
    "    'model_size_delta': {\n",
    "        'full': f\"{model_size_full - model_size_full:+.2f} ({(model_size_full - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'sampled': f\"{model_size_sampled - model_size_full:+.2f} ({(model_size_sampled - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'reduced': f\"{model_size_reduced - model_size_full:+.2f} ({(model_size_reduced - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'filtered': f\"{model_size_filtered - model_size_full:+.2f} ({(model_size_filtered - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'hpo_best_latency': f\"{model_size_best_latency - model_size_full:+.2f} ({(model_size_best_latency - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'hpo_best_f1_score': f\"{model_size_best_f1_score - model_size_full:+.2f} ({(model_size_best_f1_score - model_size_full) / model_size_full * 100:+.2f}%)\",\n",
    "        'hpo_best_balance': f\"{model_size_best_balance - model_size_full:+.2f} ({(model_size_best_balance - model_size_full) / model_size_full * 100:+.2f}%)\"\n",
    "    },\n",
    "    'training_time_abs': {\n",
    "        'full': round(train_time_full, 3),\n",
    "        'sampled': round(train_time_sampled, 3),\n",
    "        'reduced': round(train_time_reduced, 3),\n",
    "        'filtered': round(train_time_filtered, 3),\n",
    "        'hpo_best_latency': round(train_time_best_latency, 3),\n",
    "        'hpo_best_f1_score': round(train_time_best_f1_score, 3),\n",
    "        'hpo_best_balance': round(train_time_best_balance, 3)\n",
    "    },\n",
    "    'training_time_delta': {\n",
    "        'full': \"--\",\n",
    "        'sampled': f\"{train_time_sampled - train_time_full:+.6f} ({(train_time_sampled - train_time_full) / train_time_full * 100:+.2f}%)\",\n",
    "        'reduced': f\"{train_time_reduced - train_time_full:+.6f} ({(train_time_reduced - train_time_full) / train_time_full * 100:+.2f}%)\",\n",
    "        'filtered': f\"{train_time_filtered - train_time_full:+.6f} ({(train_time_filtered - train_time_full) / train_time_full * 100:+.2f}%)\",\n",
    "        'hpo_best_latency': f\"{train_time_best_latency - train_time_full:+.6f} ({(train_time_best_latency - train_time_full) / train_time_full * 100:+.2f}%)\",\n",
    "        'hpo_best_f1_score': f\"{train_time_best_f1_score - train_time_full:+.6f} ({(train_time_best_f1_score - train_time_full) / train_time_full * 100:+.2f}%)\",\n",
    "        'hpo_best_balance': f\"{train_time_best_balance - train_time_full:+.6f} ({(train_time_best_balance - train_time_full) / train_time_full * 100:+.2f}%)\"\n",
    "    },\n",
    "    'latency_abs': {\n",
    "        'full': round(latency_full, 12),\n",
    "        'sampled': round(latency_sampled, 12),\n",
    "        'reduced': round(latency_reduced, 12),\n",
    "        'filtered': round(latency_filtered, 12),\n",
    "        'hpo_best_latency': round(latency_best_latency, 12),\n",
    "        'hpo_best_f1_score': round(latency_best_f1_score, 12),\n",
    "        'hpo_best_balance': round(latency_best_balance, 12)\n",
    "    },\n",
    "    'latency_delta': {\n",
    "        'full': \"--\",\n",
    "        'sampled': f\"{latency_sampled - latency_full:+.2e} ({(latency_sampled - latency_full) / latency_full * 100:+.2f}%)\",\n",
    "        'reduced': f\"{latency_reduced - latency_full:+.2e} ({(latency_reduced - latency_full) / latency_full * 100:+.2f}%)\",\n",
    "        'filtered': f\"{latency_filtered - latency_full:+.2e} ({(latency_filtered - latency_full) / latency_full * 100:+.2f}%)\",\n",
    "        'hpo_best_latency': f\"{latency_best_latency - latency_full:+.2e} ({(latency_best_latency - latency_full) / latency_full * 100:+.2f}%)\",\n",
    "        'hpo_best_f1_score': f\"{latency_best_f1_score - latency_full:+.2e} ({(latency_best_f1_score - latency_full) / latency_full * 100:+.2f}%)\",\n",
    "        'hpo_best_balance': f\"{latency_best_balance - latency_full:+.2e} ({(latency_best_balance - latency_full) / latency_full * 100:+.2f}%)\"\n",
    "    },\n",
    "    'f1_score_abs': {\n",
    "        'full': round(f1_weighted_full, 6),\n",
    "        'sampled': round(f1_weighted_sampled, 6),\n",
    "        'reduced': round(f1_weighted_reduced, 6),\n",
    "        'filtered': round(f1_weighted_filtered, 6),\n",
    "        'hpo_best_latency': round(f1_weighted_best_latency, 6),\n",
    "        'hpo_best_f1_score': round(f1_weighted_best_f1_score, 6),\n",
    "        'hpo_best_balance': round(f1_weighted_best_balance, 6)\n",
    "    },\n",
    "    'f1_score_delta': {\n",
    "        'full': \"--\",\n",
    "        'sampled': f\"{f1_weighted_sampled - f1_weighted_full:+.6f} ({(f1_weighted_sampled - f1_weighted_full) * 100:+.2f}%)\",\n",
    "        'reduced': f\"{f1_weighted_reduced - f1_weighted_full:+.6f} ({(f1_weighted_reduced - f1_weighted_full) * 100:+.2f}%)\",\n",
    "        'filtered': f\"{f1_weighted_filtered - f1_weighted_full:+.6f} ({(f1_weighted_filtered - f1_weighted_full) * 100:+.2f}%)\",\n",
    "        'hpo_best_latency': f\"{f1_weighted_best_latency - f1_weighted_full:+.6f} ({(f1_weighted_best_latency - f1_weighted_full) * 100:+.2f}%)\",\n",
    "        'hpo_best_f1_score': f\"{f1_weighted_best_f1_score - f1_weighted_full:+.6f} ({(f1_weighted_best_f1_score - f1_weighted_full) * 100:+.2f}%)\",\n",
    "        'hpo_best_balance': f\"{f1_weighted_best_balance - f1_weighted_full:+.6f} ({(f1_weighted_best_balance - f1_weighted_full) * 100:+.2f}%)\"\n",
    "    },\n",
    "    'hpo_n_trials': {\n",
    "        'full': \"--\",\n",
    "        'sampled': \"--\",\n",
    "        'reduced': \"--\",\n",
    "        'filtered': \"--\",\n",
    "        'hpo_best_latency': completion_ratio_str,\n",
    "        'hpo_best_f1_score': completion_ratio_str,\n",
    "        'hpo_best_balance': completion_ratio_str\n",
    "    },\n",
    "    'hpo_params': {\n",
    "        'full': \"--\",\n",
    "        'sampled': \"--\",\n",
    "        'reduced': \"--\",\n",
    "        'filtered': \"--\",\n",
    "        'hpo_best_latency': round_floats_in_dict(best_params_best_latency),\n",
    "        'hpo_best_f1_score': round_floats_in_dict(best_params_best_f1_score),\n",
    "        'hpo_best_balance': round_floats_in_dict(best_params_best_balance)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5584516-1df4-48c8-b0ac-91f533296c45",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary).style \\\n",
    "    .background_gradient(cmap=\"RdYlGn_r\", subset=[\"model_size_abs\"]) \\\n",
    "    .background_gradient(cmap=\"RdYlGn_r\", subset=[\"training_time_abs\"]) \\\n",
    "    .background_gradient(cmap=\"RdYlGn_r\", subset=[\"latency_abs\"]) \\\n",
    "    .background_gradient(cmap=\"RdYlGn\", subset=[\"f1_score_abs\"]) \\\n",
    "    .format({\n",
    "        \"model_size_abs\": \"{:.2f}\",\n",
    "        \"training_time_abs\": \"{:.3f}\",\n",
    "        \"f1_score_abs\": \"{:.6f}\",\n",
    "        \"latency_abs\": \"{:.2e}\",\n",
    "    }) \\\n",
    "    .set_table_styles([\n",
    "        {\"selector\": \"th, td\", \"props\": \"border: 1px solid black; text-align: center;\"}\n",
    "    ])\n",
    "\n",
    "summary_df.to_excel(f\"{output_folder}/xgb_summary_table.xlsx\")\n",
    "\n",
    "with open(f\"{output_folder}/xgb_summary_table.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_df.to_html())\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0931325",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(f\"Global Time: {time.time() - global_start:.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
